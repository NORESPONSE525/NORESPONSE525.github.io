{
    "version": "https://jsonfeed.org/version/1",
    "title": " • All posts by \"人工智能\" category",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/06/12/AI/week6-7/",
            "url": "http://example.com/2025/06/12/AI/week6-7/",
            "title": "Week6-7",
            "date_published": "2025-06-11T16:00:00.000Z",
            "content_html": "<h1 id=\"Ch4-机器学习\"><a href=\"#Ch4-机器学习\" class=\"headerlink\" title=\"Ch4 机器学习\"></a>Ch4 机器学习</h1><h2 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h2><ul>\n<li>标注数据</li>\n<li>学习模型</li>\n<li>损失函数<br>典型的损失函数<br><img data-src=\"/figure2.png\"></li>\n</ul>\n<p>经验风险(empirical risk )</p>\n<ul>\n<li>训练集中数据产生的损失。</li>\n<li>经验风险越小说明学习模型对训练数据拟合程度越好。</li>\n</ul>\n<p>期望风险(expected risk):</p>\n<ul>\n<li>当测试集中存在无穷多数据时产生的损失。</li>\n<li>期望风险越小，学习所得模型越好。</li>\n</ul>\n<p>经验风险最小化</p>\n<p>$$\\min_{f \\in \\Phi} \\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i))$$</p>\n<p>期望风险最小化</p>\n<p>$$\\min_{f \\in \\Phi} \\int_{x \\times y} Loss(y, f(x)) P(x, y) dx dy$$</p>\n<p>模型泛化能力与经验风险、期望风险的关系</p>\n<table>\n<thead>\n<tr>\n<th>经验风险小（训练集上表现好）</th>\n<th>期望风险小（测试集上表现好）</th>\n<th>泛化能力强</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>经验风险小（训练集上表现好）</td>\n<td>期望风险大（测试集上表现不好）</td>\n<td>过学习（模型过于复杂）</td>\n</tr>\n<tr>\n<td>经验风险大（训练集上表现不好）</td>\n<td>期望风险大（测试集上表现不好）</td>\n<td>欠学习</td>\n</tr>\n<tr>\n<td>经验风险大（训练集上表现不好）</td>\n<td>期望风险小（测试集上表现好）</td>\n<td>“神仙算法”或“黄粱美梦”</td>\n</tr>\n</tbody></table>\n<p>结构风险最小化 (structural risk minimization)</p>\n<p>为了防止过拟合，在经验风险上加上表示模型复杂度的正则化项 (regularizer) 或惩罚项 (penalty term):</p>\n<p>$$\\min_{f \\in \\Phi} \\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i)) + \\lambda J(f)$$</p>\n<ul>\n<li>经验风险: $\\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i))$</li>\n<li>模型复杂度: $\\lambda J(f)$</li>\n</ul>\n<p>监督学习方法又可以分为 生成方法 (generative approach) 和 判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative model)和判别模型(discriminative model)<br><img data-src=\"/figure1.png\"></p>\n<h1 id=\"回归分析\"><a href=\"#回归分析\" class=\"headerlink\" title=\"回归分析\"></a>回归分析</h1><h2 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h2><ul>\n<li>一元线性回归</li>\n</ul>\n<p>$$y_i &#x3D; ax_i + b \\quad (1 \\leq i \\leq n)$$</p>\n<p>$$a &#x3D; \\frac{\\sum_{i&#x3D;1}^{n} x_i y_i - n \\bar{x} \\bar{y}}{\\sum_{i&#x3D;1}^{n} x_i^2 - n \\bar{x}^2}$$</p>\n<p>$$b &#x3D; \\bar{y} - a \\bar{x}$$</p>\n<ul>\n<li><p>多元线性回归<br>$$f(x_i) &#x3D; a_0 + \\sum_{j&#x3D;1}^{D} a_j x_{i,j} &#x3D; a_0 + \\mathbf{a}^T \\mathbf{x}_i$$<br>a是要求的参数，x是输入的数据，f是预测值。<br>为了方便，使用矩阵来表示所有的训练数据和数据标签。<br>$$X &#x3D; [x_1, …, x_m], \\quad y &#x3D; [y_1, …, y_m]$$<br>最小化均方误差得到：<br>$$a &#x3D; (XX^T)^{-1}X^Ty$$</p>\n</li>\n<li><p>逻辑斯蒂回归&#x2F;对数几率回归<br>线性回归一个明显的问题是对离群点导致模型建模不稳定，使结果有偏，为了缓解这个问题（特别是在二分类场景中）带来的影响，可考虑逻辑斯蒂回归<br>逻辑斯蒂回归就是在回归模型中引入 sigmoid函数的一种非线性回归模型</p>\n</li>\n</ul>\n<h2 id=\"逻辑斯蒂回归-Logistic-Regression\"><a href=\"#逻辑斯蒂回归-Logistic-Regression\" class=\"headerlink\" title=\"逻辑斯蒂回归 (Logistic Regression)\"></a>逻辑斯蒂回归 (Logistic Regression)</h2><p>逻辑斯蒂回归（logistic regression）就是在回归模型中引入 sigmoid 函数的一种非线性回归模型。Logistic 回归模型可如下表示：</p>\n<p>$$ y &#x3D; \\frac{1}{1 + e^{-z}} &#x3D; \\frac{1}{1 + e^{-(w^T x + b)}} $$<br>其中 $y \\in (0, 1)$，$z &#x3D; w^T x + b$。<br>这里 $\\frac{1}{1 + e^{-z}}$ 是 sigmoid 函数，$x \\in \\mathbb{R}^d$ 是输入数据，$w \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$ 是回归函数的参数。</p>\n<p>逻辑斯蒂回归多用于&#x3D;&#x3D;二分类&#x3D;&#x3D;问题<br>Sigmoid 函数将任意实数映射到区间(0,1)，这正好符合“概率”的取值范围，所以函数的输出y可以被解释为输入数据x属于正例的概率<br>因此我们可以将输出 y 解释为：</p>\n<blockquote>\n<p>在给定输入特征 x 的条件下，该样本属于正类（例如类别 1）的概率。<br>即：<br>$$<br>y &#x3D; P(y &#x3D; 1 \\mid x)<br>$$<br>如果 $P(y&#x3D;1|x)$ 表示给定输入 $x$ 属于正类的概率，则 $1 - P(y&#x3D;1|x)$ 表示属于负类的概率。<br>$\\frac{P(y&#x3D;1|x)}{1 - P(y&#x3D;1|x)}$ 就是正类相对于负类的优势比。所以&gt;1就归为正类，反之就是负类。</p>\n</blockquote>\n<p>$$<br>\\log \\frac{P(y&#x3D;1|x)}{P(y&#x3D;0|x)} &#x3D; {w^T x + b} &gt; \\log{1} &#x3D; 0<br>$$<br>从这里可以看出，logistic回归本质上是一个线性模型。在预测时，可以计算线性函数$w^T x + b$取值是否大于0来判断输入数据x的类别归属</p>\n<p>为了找到最优参数w和b，我们使用最大似然估计，假设每个样本独立同分布，则<br>……<br>公式懒得敲了，</p>\n<p>为什么基于相关性的方法可能会导致模型的不可解释性和不稳定性</p>\n<ul>\n<li>因果特征和非因果特征</li>\n<li>Making V⊥Y: 最终目标是让非因果特征 V 与输出 Y 独立，即消除虚假相关性，使得模型更加稳定和可解释</li>\n</ul>\n<h1 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h1><p>决策树是一种通过树形结构来进行分类的方法</p>\n<ul>\n<li>信息熵（entropy）是度量样本集合纯度最常用的一种指标<br>假设有一个K个信息（类别），其组成了集合样本D，记第k个信息（类别）发生的概率为$p_k (1 \\leq k \\leq K)$。如下定义这K个信息的信息熵：</li>\n</ul>\n<p>$$Ent(D) &#x3D; -\\sum_{k&#x3D;1}^{K} p_k \\log_2 p_k$$</p>\n<p>&#x3D;&#x3D;$Ent(D)$值越小，表示D包含的信息越确定，也称D的纯度越高。&#x3D;&#x3D;所有$p_k$累加起来的和为1。</p>\n<ul>\n<li>信息增益:衡量使用某个属性进行划分后，数据集不确定性减少的程度<br>得到信息熵后可以进一步计算信息增益：<br>$$Gain(D, A) &#x3D; Ent(D) - \\sum_{i&#x3D;1}^{n} \\frac{|D_i|}{|D|} Ent(D_i)$$<br><img data-src=\"/f3.png\"><br><img data-src=\"/f4.png\"><br>ID3决策树学习算法[Quinlan, 1986]以信息增益为准则来选择划分属性<br>目标：通过不断划分，使得每个子集尽可能“纯净”，即子集内的样本属于同一类</li>\n</ul>\n<p>信息熵（和上面的一样的）<br>$$<br>info &#x3D; -\\sum_{i&#x3D;1}^{n} \\frac{|D_i|}{|D|} \\log_2 \\frac{|D_i|}{|D|}<br>$$</p>\n<p>增益率（Gain-ratio）：</p>\n<p>$$<br>Gain-ratio &#x3D; \\frac{Gain(D, A)}{info}<br>$$<br>存在的问题：增益率准则对可取数目较少的属性有所偏好</p>\n<p>另一种计算更简的度量指标是如下的 Gini 指数（基尼指数）：</p>\n<p>$$<br>Gini(D) &#x3D; 1 - \\sum_{k&#x3D;1}^{K} p_k^2<br>$$</p>\n<p>相对于信息熵的计算 $E(D) &#x3D; -\\sum_{k&#x3D;1}^{K} p_k \\log_2 p_k$，不用计算对数 log，计算更为简易。</p>\n<h2 id=\"连续属性离散化\"><a href=\"#连续属性离散化\" class=\"headerlink\" title=\"连续属性离散化\"></a>连续属性离散化</h2><ol>\n<li><p>确定连续属性的取值范围，确定划分点集合<br>考虑包含 n-1 个元素的候选划分点集合：<br>$$<br>T_a &#x3D; \\left{ \\frac{a^i + a^{i+1}}{2} ,\\middle|, 1 \\leq i \\leq n - 1 \\right}<br>$$<br>这里的每个候选划分点是相邻两个取值的中点，即区间 $[a^i, a^{i+1})$ 的中位点 $\\frac{a^i + a^{i+1}}{2}$</p>\n</li>\n<li><p>计算信息增益<br>$$<br>\\text{Gain}(D, a, t) &#x3D; \\text{Ent}(D) - \\sum_{\\lambda \\in {-, +}} \\frac{|D_t^\\lambda|}{|D|} \\cdot \\text{Ent}(D_t^\\lambda)<br>$$<br>计算每个划分点的信息增益率，选择信息增益最大的划分点</p>\n</li>\n</ol>\n<p>+++info example<br>;;;id3 example<br>给定数据点及其对应的类别标签如下：</p>\n<ul>\n<li>$a_1 &#x3D; 1$ -&gt; 类别为 0</li>\n<li>$a_2 &#x3D; 3$ -&gt; 类别为 1</li>\n<li>$a_3 &#x3D; 5$ -&gt; 类别为 0</li>\n<li>$a_4 &#x3D; 7$ -&gt; 类别为 1</li>\n<li>$a_5 &#x3D; 9$ -&gt; 类别为 0</li>\n</ul>\n<p>因此，我们的数据集 $D$ 是 ${1, 3, 5, 7, 9}$，对应的类别标签分别为 ${0, 1, 0, 1, 0}$。</p>\n<p>第一步：计算原始数据集的信息熵</p>\n<p>$$<br>Ent(D) &#x3D; -\\left( p_0 \\log_2 p_0 + p_1 \\log_2 p_1 \\right)<br>$$</p>\n<p>其中，$p_0 &#x3D; \\frac{3}{5}$，$p_1 &#x3D; \\frac{2}{5}$，则：</p>\n<p>$$<br>Ent(D) &#x3D; -\\left( \\frac{3}{5} \\log_2 \\frac{3}{5} + \\frac{2}{5} \\log_2 \\frac{2}{5} \\right) \\approx 0.971<br>$$</p>\n<p>第二步：确定候选划分点集合</p>\n<p>根据公式 $T_a &#x3D; \\left{ \\frac{a^i + a^{i+1}}{2} ,\\middle|, 1 \\leq i \\leq n - 1 \\right}$，我们得到候选划分点集合：</p>\n<p>$$<br>T_a &#x3D; {2, 4, 6, 8}<br>$$</p>\n<p>第三步：计算每个候选划分点的信息增益</p>\n<p>以划分点 $t &#x3D; 4$ 为例：</p>\n<ul>\n<li>$D_t^{-} &#x3D; {1, 3}$，类别为 ${0, 1}$</li>\n<li>$D_t^{+} &#x3D; {5, 7, 9}$，类别为 ${0, 1, 0}$</li>\n</ul>\n<p>计算这两个子集的熵：</p>\n<ul>\n<li>$$Ent(D_t^{-}) &#x3D; -\\left( \\frac{1}{2} \\log_2 \\frac{1}{2} + \\frac{1}{2} \\log_2 \\frac{1}{2} \\right) &#x3D; 1$$</li>\n<li>$$Ent(D_t^{+}) &#x3D; -\\left( \\frac{2}{3} \\log_2 \\frac{2}{3} + \\frac{1}{3} \\log_2 \\frac{1}{3} \\right) \\approx 0.918$$</li>\n</ul>\n<p>计算信息增益：</p>\n<p>$$<br>Gain(D, a, t&#x3D;4) &#x3D; Ent(D) - \\left( \\frac{|D_t^{-}|}{|D|} \\cdot Ent(D_t^{-}) + \\frac{|D_t^{+}|}{|D|} \\cdot Ent(D_t^{+}) \\right)<br>$$</p>\n<p>代入数值：</p>\n<p>$$<br>Gain(D, a, t&#x3D;4) &#x3D; 0.971 - \\left( \\frac{2}{5} \\cdot 1 + \\frac{3}{5} \\cdot 0.918 \\right) \\approx 0.029<br>$$</p>\n<p>重复上述过程，对所有划分点 $t &#x3D; 2, 4, 6, 8$ 进行类似计算，并选择使 $Gain(D, a, t)$ 最大的那个作为最优划分点。<br>;;;<br>+++</p>\n<h1 id=\"线性区别分析-LDA-FDA\"><a href=\"#线性区别分析-LDA-FDA\" class=\"headerlink\" title=\"线性区别分析 (LDA&#x2F;FDA)\"></a>线性区别分析 (LDA&#x2F;FDA)</h1><p>线性判别分析(linear discriminant analysis， LDA)是一种基于监督学习的降维方法，也称为Fisher线性判别分析(fisher’s discriminant analysis，FDA),对于一组具有标签信息的高维数据样本，LDA利用其类别信息，将其线性投影到一个低维空间上，在低维空间中同一类别样本尽可能靠近，不同类别样本尽可能彼此远离。</p>\n<ol>\n<li>计算数据样本集中每个类别样本的均值</li>\n<li>计算类内散度矩阵$S_w$和类间散度矩阵$S_b$</li>\n<li>根据$S_w^{-1}S_bW&#x3D;\\lambda W$来求解$S_w^{-1}S_b$所对应前$r$个最大特征值所对应特征向量$(w_1,w_2,…,w_r)$，构成矩阵W</li>\n<li>通过矩阵$W$将每个样本映射到低维空间，实现特征降维。</li>\n</ol>\n<p>具体不想看，考到就给了</p>\n<h1 id=\"Ada-Boosting\"><a href=\"#Ada-Boosting\" class=\"headerlink\" title=\"Ada Boosting\"></a>Ada Boosting</h1><p>。。看不懂懒得看</p>\n<h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a>支持向量机</h1><h1 id=\"生成学习模型\"><a href=\"#生成学习模型\" class=\"headerlink\" title=\"生成学习模型\"></a>生成学习模型</h1>",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/06/11/AI/week4-5/",
            "url": "http://example.com/2025/06/11/AI/week4-5/",
            "title": "Week4-5",
            "date_published": "2025-06-10T16:00:00.000Z",
            "content_html": "<p>#Ch3 搜索算法</p>\n<h2 id=\"无信息搜索\"><a href=\"#无信息搜索\" class=\"headerlink\" title=\"无信息搜索\"></a>无信息搜索</h2><p>BFS DFS 略</p>\n<h2 id=\"启发式搜索\"><a href=\"#启发式搜索\" class=\"headerlink\" title=\"启发式搜索\"></a>启发式搜索</h2><ul>\n<li>贪婪优先搜索<ul>\n<li>每次取最短的；缺点：不一定是最优的</li>\n<li>时间和空间复杂度均为 $O(b_m)$，b是搜索树分支因子，m是最大深度<br><img data-src=\"/figure1.png\"><br>:::info<br>每次取当前节点的下一个节点到终点中直线距离最短的<br>:::</li>\n</ul>\n</li>\n<li>A*算法<ul>\n<li>评价函数：f(n) &#x3D; g(n) + h(n)</li>\n<li>代价函数 g(n) 表示从起始结点到结点n的开销代价值</li>\n<li>启发函数 h(n) 表示从结点n到目标结点路径中所估算的最小开销代价值。</li>\n<li>评价函数 f(n) 可视为经过结点n、具有最小开销代价值的路径。<ul>\n<li>在最短路径问题中，g(?)为当前选择的路径的实际距离，即从上一个节点到下一个节点的实际距离，?(?)为下一个节点到目标城市的直线距离。每一次搜索，下一个节点选择与此刻城市连接的所有节点中，g(?)+?(?)最小的城市节点。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>:::info<br>取（当前节点到下一节点的距离+下一节点到目标城市的距离）最短的<br>:::<br>A*算法的完备性和最优性取决于搜索问题和启发函数的性质<br>一个良好的启发函数需要满足:可容性（admissible）;一致性（consistency）<br>如果启发函数是可容的，那么树搜索的A*算法满足最优性(最优性:搜索算法是否能保证找到的第一个解是最优解)<br>满足一致性条件的启发函数一定满足可容性条件，反之不一定</p>\n<h2 id=\"对抗搜索\"><a href=\"#对抗搜索\" class=\"headerlink\" title=\"对抗搜索\"></a>对抗搜索</h2><ul>\n<li><p>最小最大搜索（minimax）</p>\n<ul>\n<li>最小最大搜索是一个在你和对手轮流行动的情况下，为你自己寻找最优策略的算法。</li>\n<li>算法：略</li>\n<li>时间复杂度：$O(b^m)$</li>\n<li>空间复杂度：$O(bm)$</li>\n</ul>\n</li>\n<li><p>\\alpha-\\beta剪枝</p>\n<ul>\n<li>Minimax 会穷举整个博弈树，但我们可以用剪枝技巧跳过一些无用分支，让它跑得更快</li>\n<li>max层的下界取下一层（上界）里面最大的；min层的上界取下一层（下界）里面最小的<br>懒得写直接看例子：<br><img data-src=\"/figure2.png\"><br> Alpha-Beta 剪枝算法什么时候扩展的结点数量最少？</li>\n<li>每一层最左端结点的所有孩子结点均被访问，其他节点仅有最左端孩子结点被访问、其他孩子结点被剪枝。<br> 如果一个节点导致了其兄弟节点被剪枝，可知其孩子节点必然被扩展。</li>\n<li>最优效率下时间复杂度：$O(b^{m&#x2F;2})$  (或者m+1);最差的就是完全没剪枝和minimax一样</li>\n</ul>\n</li>\n<li><p>蒙特卡洛树搜索</p>\n<ul>\n<li>选择(UCB)、扩展(随机)、模拟(随机)、反向传播</li>\n<li>悔值函数<br>:::info<br>没完全懂，后面再回来研究<br>:::</li>\n</ul>\n</li>\n</ul>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/05/20/AI/week1/",
            "url": "http://example.com/2025/05/20/AI/week1/",
            "title": "Week1",
            "date_published": "2025-05-19T16:00:00.000Z",
            "content_html": "<blockquote>\n<p>2025-2026春夏人工智能课程笔记</p>\n</blockquote>\n<h1 id=\"Ch1-绪论\"><a href=\"#Ch1-绪论\" class=\"headerlink\" title=\"Ch1 绪论\"></a>Ch1 绪论</h1><ul>\n<li>人工智能求解：<ul>\n<li>以符号主义为核心的逻辑推理：将概念（如命题等）符号化，从若干判断（前提）出发得到新判断（结论）</li>\n<li>以问题求解为核心的探寻搜索:探寻搜索依据已有信息来寻找满足约束条件的待求解问题的答案</li>\n<li>以数据驱动为核心的机器学习:从数据中发现数据所承载语义（如概念）的内在模式</li>\n<li>以行为主义为核心的强化学习:根据环境所提供的奖罚反馈来学习所处状态可施加的最佳行动，在“探索（未知空间）-利用（已有经验）（exploration vs. exploitation）”之间寻找平衡，完成某个序列化任务，具备自我学习能力</li>\n<li>以博弈对抗为核心的群体智能:从“数据拟合”优化解的求取向“均衡解”的求取迈进</li>\n</ul>\n</li>\n</ul>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/05/20/AI/week2-3/",
            "url": "http://example.com/2025/05/20/AI/week2-3/",
            "title": "Week2-3",
            "date_published": "2025-05-19T16:00:00.000Z",
            "content_html": "<h1 id=\"Ch2-知识表达与推理\"><a href=\"#Ch2-知识表达与推理\" class=\"headerlink\" title=\"Ch2 知识表达与推理\"></a>Ch2 知识表达与推理</h1><h2 id=\"命题逻辑\"><a href=\"#命题逻辑\" class=\"headerlink\" title=\"命题逻辑\"></a>命题逻辑</h2><p><img data-src=\"/img1.png\"><br>真值表：<br><img data-src=\"/img2.png\"></p>\n<blockquote>\n<p>“条件”命题联结词中前提为假时命题结论永远为真，bi-conditional只有两个都是true或者都是false才是true<br>逻辑等价：给定命题p和命题q，如果&#x3D;&#x3D;p和q在所有情况下都具有同样真假结果&#x3D;&#x3D;，那么p和q在逻辑上等价，一般用 $\\equiv$ 来表示，即p $\\equiv$ q。<br>判断逻辑等价：画真值表<br>逻辑等价式：<br><img data-src=\"/img3.jpg\"><br><img data-src=\"/img4.png\"></p>\n</blockquote>\n<ul>\n<li>normal form<ul>\n<li>有限个简单合取式构成的析取式称为析取(or)范式</li>\n<li>由有限个简单析取式构成的合取式称为合取(and)范式</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"谓词逻辑\"><a href=\"#谓词逻辑\" class=\"headerlink\" title=\"谓词逻辑\"></a>谓词逻辑</h2><ul>\n<li>全称量词与存在量词</li>\n<li>约束变元、自由变元<br>:::info<br>在约束变元相同的情况下，量词的运算满足分配律：全称量词对析取没有分配律、存在量词对合取没有分配律<br>:::<br>$$\\begin{aligned}<br>(\\forall x)(A(x) \\lor B(x)) \\equiv (\\forall x)A(x) \\lor (\\forall x)B(x) 不成立<br>\\end{aligned}$$</li>\n</ul>\n<p>$$\\begin{aligned}<br>(\\forall x)(A(x) \\land B(x)) \\equiv (\\forall x)A(x) \\land (\\forall x)B(x) 成立<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(A(x) \\lor B(x)) \\equiv (\\exists x)A(x) \\lor (\\exists x)B(x) 成立<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(A(x) \\land B(x)) \\equiv (\\exists x)A(x) \\land (\\exists x)B(x) 不成立<br>\\end{aligned}$$<br>:::info<br>当公式中存在多个量词时，若多个量词都是全称量词或者都是存在量词，则量词的位置可以互换；若多个量词中既有全称量词又有存在量词，则量词的位置不可以随意互换<br>:::<br>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\forall y)(\\forall x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(\\exists y)A(x, y) \\equiv (\\exists y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\exists y)(\\forall x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\exists x)(\\forall y)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists y)(\\forall x)A(x, y) \\equiv (\\forall x)(\\exists y)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(\\forall y)A(x, y) \\equiv (\\forall y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\exists y)A(x, y) \\equiv (\\exists y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall y)(\\exists x)A(x, y) \\equiv (\\exists x)(\\exists y)A(x, y)<br>\\end{aligned}$$</p>\n<ul>\n<li>利用谓词逻辑进行推理<ul>\n<li>全称量词消去： $(\\forall x) A(x) \\equiv A(y)$</li>\n<li>全称量词引入： $A(y) \\equiv (\\forall x) A(x)$</li>\n<li>存在量词消去： $(\\exists x) A(x) \\equiv A(c)$</li>\n<li>存在量词引入： $A(c) \\equiv (\\exists x) A(x)$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"知识图谱推理\"><a href=\"#知识图谱推理\" class=\"headerlink\" title=\"知识图谱推理\"></a>知识图谱推理</h2><ul>\n<li>知识图谱可视为包含多种关系的图。在图中，每个节点是一个实体（如人名、地名、事件和活动等），任意两个节点之间的边表示这两个节点之间存在的关系。</li>\n<li>可将知识图谱中任意两个相连节点及其连接边表示成一个三元组（triplet）,即 (left_node, relation, right_node)<br>两类代表性方法：</li>\n<li>归纳逻辑程序设计 (inductive logic programming，ILP)算法</li>\n<li>路径排序算法（path ranking algorithm, PRA）</li>\n</ul>\n<p>ILP: 一阶归纳学习FOIL（First Order Inductive Learner）<br>推理手段: 正例集合 + 反例集合 + 背景知识样例 ⟹ 目标谓词作为结论的推理规则<br><img data-src=\"/img5.png\"><br>懒得写了，看ppt吧<br><img data-src=\"/img6.png\"><br>推理规则覆盖所有正例且不覆盖任何反例的时候算法结束</p>\n<p>PRA: 路径排序算法<br><img data-src=\"/img7.png\"><br>(4)的意思是看两个实体能不能通过(3)的关系从第一个走到第二个。<br>后面的1表示正例，-1表示负例。</p>\n<h2 id=\"概率图推理\"><a href=\"#概率图推理\" class=\"headerlink\" title=\"概率图推理\"></a>概率图推理</h2><p>贝叶斯网络<br>马尔科夫逻辑网络</p>\n<h2 id=\"因果推理\"><a href=\"#因果推理\" class=\"headerlink\" title=\"因果推理\"></a>因果推理</h2><p>因果定义：变量X是变量Y的原因，当且仅当保持其它所有变量不变的情况下，改变X的值能导致Y的值发生变化。<br>因果效应：因变量X改变一个单位时，果变量Y的变化程度</p>\n<p>因果图是有向无环图</p>\n<p>结构因果模型：结构因果模型由两组变量集合U和V以及一组函数f组成。其中，f是根据模型中其他变量取值而给V中每一个变量赋值的函数<br>结构因果模型中的原因：如果变量X出现在给变量X赋值的函数中，如$Y &#x3D; f(X) + \\epsilon$，则X是Y的直接原因<br>因果图中的联合概率分布：直接看图<br><img data-src=\"/img8.png\"><br>因果图的基本结构：</p>\n<ul>\n<li>链结构<br>  - <img data-src=\"/img9.png\"><br>  - 对于变量X和Y，若X和Y之间只有一条单向的路径，变量Z是截断(intercept)该路径的集合中的任一变量，则在给定Z时，X和Y条件独立。</li>\n</ul>\n<p>$$<br>P(X, Y | Z) &#x3D; P(X | Z)P(Y | Z)<br>$$</p>\n<ul>\n<li>分连结构<br>  - <img data-src=\"/img10.png\"></li>\n</ul>\n<p>$$<br>P(X, Y | Z) &#x3D; \\frac {P(X, Y, Z)}{P(Z)} &#x3D; \\frac {P(X | Z)P(Y | Z)P(Z)}{P(Z)} &#x3D; P(X | Z)P(Y | Z)<br>$$</p>\n<ul>\n<li>汇联结构<br>  - <img data-src=\"/img11.png\"></li>\n</ul>\n<p>$$<br>P(X, Y | Z) &#x3D; \\frac{P(X, Y, Z)} {P(Z)} &#x3D; \\frac {P(X, Y, Z)}{P(Z)} &#x3D; \\frac {P(X)P(Y)P(Z&#x2F;X, Y)}{P(Z)} \\neq P(X | Z)P(Y | Z)<br>$$</p>\n<h3 id=\"D-分离-directional-separation-d-separation-，可用于判断任意两个节点的相关性和独立性\"><a href=\"#D-分离-directional-separation-d-separation-，可用于判断任意两个节点的相关性和独立性\" class=\"headerlink\" title=\"D-分离(directional separation, d-separation)，可用于判断任意两个节点的相关性和独立性\"></a>D-分离(directional separation, d-separation)，可用于判断任意两个节点的相关性和独立性</h3><ul>\n<li>限定集：已知或观察到的变量集合（给定的变量集合）</li>\n<li>路径p被限定集Z阻塞(block)当且仅当：<ul>\n<li>(1) 路径p含有链结构A → B → C或分连结构A ← B → C且中间节点B在Z中，或</li>\n<li>(2) 路径p含有汇连结构A → B ← C且汇连节点B及其后代都不在Z中。</li>\n<li>若Z阻塞了节点X和节点Y之间的每一条路径，则称给定Z时，X和Y是D-分离，即给定Z时，X和Y条件独立</li>\n<li>&#x3D;&#x3D;链式、分连中间节点在，汇联中间节点和后代不在则D-分离&#x3D;&#x3D;</li>\n</ul>\n</li>\n</ul>\n<p>因果定义：变量X是变量Y的原因，当且仅当保持其它所有变量不变的情况下，改变X的值能导致Y的值发生变化。<br>因果效应：因变量X改变一个单位时，果变量Y的变化程度因果推理的两个关键因素：</p>\n<ul>\n<li>改变因变量T</li>\n<li>保证其它变量不变<br>干预：干预(intervention)指的是固定(fix)系统中的变量，然后改变系统，观察其他变量的变化。<br>为了与X自然取值x时进行区分，在对X进行干预时，引入“do算子”(do-calculus)，记作do(X &#x3D; x)。<br>因此，P(Y &#x3D; y|X &#x3D; x)表示的是当发现X &#x3D; x时，Y&#x3D; y的概率；而P(Y &#x3D; y|do(X &#x3D;x))表示的是对X进行干预，固定其值为x时，Y &#x3D; y的概率。<br>用统计学的术语来说，P(Y &#x3D; y|X &#x3D; x)反映的是在取值为x的个体X上，Y的总体分布；而P(Y &#x3D; y|do(X &#x3D;x))反映的是如果将每一个X取值都固定为x时，Y的总体分布。</li>\n</ul>\n<p>因果效应差&#x2F;平均因果效应 (ACE)  懒得写了看图吧<br><img data-src=\"/img12.png\"><br><img data-src=\"/img13.png\"><br>计算因果效应的关键在于计算操纵概率(manipulatedprobability) $P_m$<br>调整公式：<br>$$<br>P(Y &#x3D; y \\mid do(X &#x3D; x)) &#x3D; \\sum_z P(Y &#x3D; y \\mid X &#x3D; x, Z &#x3D; z) \\cdot P(Z &#x3D; z)<br>$$<br>对于Z的每一个取值z，计算X和Y的条件概率并取均值<br>+++info example<br>;;;id3 例题<br>假设我们研究以下变量：</p>\n<ul>\n<li>X：是否服药  <ul>\n<li>$X &#x3D; 1$：服药  </li>\n<li>$X &#x3D; 0$：不服药</li>\n</ul>\n</li>\n<li>Y：是否康复  <ul>\n<li>$Y &#x3D; 1$：康复  </li>\n<li>$Y &#x3D; 0$：未康复</li>\n</ul>\n</li>\n<li>Z：性别  <ul>\n<li>$Z &#x3D; 0$：男  </li>\n<li>$Z &#x3D; 1$：女<br>我们知道性别会影响：</li>\n</ul>\n</li>\n<li>是否选择服药（比如男性更倾向于尝试新药）</li>\n<li>康复率（比如女性可能有更强的免疫力）<br>因此，性别 Z 是一个混杂变量，需要在分析中进行控制。<br>已知：<table>\n<thead>\n<tr>\n<th>Z（性别）</th>\n<th>P(Z)</th>\n<th>P(Y&#x3D;1 | X&#x3D;1, Z)</th>\n<th>P(Y&#x3D;1 | X&#x3D;0, Z)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>男（0）</td>\n<td>0.6</td>\n<td>0.7</td>\n<td>0.4</td>\n</tr>\n<tr>\n<td>女（1）</td>\n<td>0.4</td>\n<td>0.5</td>\n<td>0.3</td>\n</tr>\n<tr>\n<td>我们想知道：</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>如果强制所有人都服药（即 $do(X&#x3D;1)$），整体康复率是多少？</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>也就是要计算：</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$$</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>P(Y&#x3D;1 \\mid do(X&#x3D;1))</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$$</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>;;;</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>;;;id3 答案</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>根据调整公式：</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<p>$$<br>P(Y&#x3D;1 \\mid do(X&#x3D;1)) &#x3D; \\sum_z P(Y&#x3D;1 \\mid X&#x3D;1, Z&#x3D;z) \\cdot P(Z&#x3D;z)<br>$$</p>\n<p>代入数据计算</p>\n<p>$$<br>P(Y&#x3D;1 \\mid do(X&#x3D;1)) &#x3D; P(Y&#x3D;1 \\mid X&#x3D;1, Z&#x3D;0) \\cdot P(Z&#x3D;0) + P(Y&#x3D;1 \\mid X&#x3D;1, Z&#x3D;1) \\cdot P(Z&#x3D;1)<br>$$</p>\n<p>$$<br>&#x3D; 0.7 \\times 0.6 + 0.5 \\times 0.4 &#x3D; 0.42 + 0.2 &#x3D; 0.62<br>$$<br>+++</p>\n<p>(因果效应)给定因果图G，PA表示X的父节点集合，则X对Y的因果效应为<br>$$<br>P(Y&#x3D;y \\mid do(X&#x3D;x)) &#x3D; \\sum_z P(Y&#x3D;y \\mid X&#x3D;x, PA&#x3D;z) \\cdot P(PA&#x3D;z)<br>$$<br>后门调整：<br>不写了</p>\n",
            "tags": [
                "人工智能"
            ]
        }
    ]
}