<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title> • Posts by &#34;人工智能&#34; tag</title>
        <link>http://example.com</link>
        <description></description>
        <language>en</language>
        <pubDate>Thu, 12 Jun 2025 00:00:00 +0800</pubDate>
        <lastBuildDate>Thu, 12 Jun 2025 00:00:00 +0800</lastBuildDate>
        <category>人工智能</category>
        <category>计算机网络</category>
        <category>计算机组成</category>
        <category>数据的表示与运算</category>
        <category>编译原理</category>
        <category>词法分析</category>
        <category>语法分析</category>
        <category>抽象语法</category>
        <category>语义分析</category>
        <item>
            <guid isPermalink="true">http://example.com/2025/06/12/AI/week6-7/</guid>
            <title>Week6-7</title>
            <link>http://example.com/2025/06/12/AI/week6-7/</link>
            <category>人工智能</category>
            <pubDate>Thu, 12 Jun 2025 00:00:00 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;Ch4-机器学习&#34;&gt;&lt;a href=&#34;#Ch4-机器学习&#34; class=&#34;headerlink&#34; title=&#34;Ch4 机器学习&#34;&gt;&lt;/a&gt;Ch4 机器学习&lt;/h1&gt;&lt;h2 id=&#34;监督学习&#34;&gt;&lt;a href=&#34;#监督学习&#34; class=&#34;headerlink&#34; title=&#34;监督学习&#34;&gt;&lt;/a&gt;监督学习&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;标注数据&lt;/li&gt;
&lt;li&gt;学习模型&lt;/li&gt;
&lt;li&gt;损失函数&lt;br&gt;典型的损失函数&lt;br&gt;&lt;img data-src=&#34;/figure2.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;经验风险(empirical risk )&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练集中数据产生的损失。&lt;/li&gt;
&lt;li&gt;经验风险越小说明学习模型对训练数据拟合程度越好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;期望风险(expected risk):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当测试集中存在无穷多数据时产生的损失。&lt;/li&gt;
&lt;li&gt;期望风险越小，学习所得模型越好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;经验风险最小化&lt;/p&gt;
&lt;p&gt;$\min_{f \in \Phi} \frac{1}{n} \sum_{i&amp;#x3D;1}^{n} Loss(y_i, f(x_i))$&lt;/p&gt;
&lt;p&gt;期望风险最小化&lt;/p&gt;
&lt;p&gt;$\min_{f \in \Phi} \int_{x \times y} Loss(y, f(x)) P(x, y) dx dy$&lt;/p&gt;
&lt;p&gt;模型泛化能力与经验风险、期望风险的关系&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;经验风险小（训练集上表现好）&lt;/th&gt;
&lt;th&gt;期望风险小（测试集上表现好）&lt;/th&gt;
&lt;th&gt;泛化能力强&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;经验风险小（训练集上表现好）&lt;/td&gt;
&lt;td&gt;期望风险大（测试集上表现不好）&lt;/td&gt;
&lt;td&gt;过学习（模型过于复杂）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;经验风险大（训练集上表现不好）&lt;/td&gt;
&lt;td&gt;期望风险大（测试集上表现不好）&lt;/td&gt;
&lt;td&gt;欠学习&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;经验风险大（训练集上表现不好）&lt;/td&gt;
&lt;td&gt;期望风险小（测试集上表现好）&lt;/td&gt;
&lt;td&gt;“神仙算法”或“黄粱美梦”&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;结构风险最小化 (structural risk minimization)&lt;/p&gt;
&lt;p&gt;为了防止过拟合，在经验风险上加上表示模型复杂度的正则化项 (regularizer) 或惩罚项 (penalty term):&lt;/p&gt;
&lt;p&gt;$$\min_{f \in \Phi} \frac{1}{n} \sum_{i&amp;#x3D;1}^{n} Loss(y_i, f(x_i)) + \lambda J(f)$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;经验风险: $\frac{1}{n} \sum_{i&amp;#x3D;1}^{n} Loss(y_i, f(x_i))$&lt;/li&gt;
&lt;li&gt;模型复杂度: $\lambda J(f)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;监督学习方法又可以分为 生成方法 (generative approach) 和 判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative model)和判别模型(discriminative model)&lt;br&gt;&lt;img data-src=&#34;/figure1.png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;回归分析&#34;&gt;&lt;a href=&#34;#回归分析&#34; class=&#34;headerlink&#34; title=&#34;回归分析&#34;&gt;&lt;/a&gt;回归分析&lt;/h1&gt;&lt;h2 id=&#34;线性回归&#34;&gt;&lt;a href=&#34;#线性回归&#34; class=&#34;headerlink&#34; title=&#34;线性回归&#34;&gt;&lt;/a&gt;线性回归&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;一元线性回归&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$y_i &amp;#x3D; ax_i + b \quad (1 \leq i \leq n)$$&lt;/p&gt;
&lt;p&gt;$$a &amp;#x3D; \frac{\sum_{i&amp;#x3D;1}^{n} x_i y_i - n \bar{x} \bar{y}}{\sum_{i&amp;#x3D;1}^{n} x_i^2 - n \bar{x}^2}$$&lt;/p&gt;
&lt;p&gt;$$b &amp;#x3D; \bar{y} - a \bar{x}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;多元线性回归&lt;br&gt;$$f(x_i) &amp;#x3D; a_0 + \sum_{j&amp;#x3D;1}^{D} a_j x_{i,j} &amp;#x3D; a_0 + \mathbf{a}^T \mathbf{x}_i$$&lt;br&gt;a是要求的参数，x是输入的数据，f是预测值。&lt;br&gt;为了方便，使用矩阵来表示所有的训练数据和数据标签。&lt;br&gt;$$X &amp;#x3D; [x_1, …, x_m], \quad y &amp;#x3D; [y_1, …, y_m]$$&lt;br&gt;最小化均方误差得到：&lt;br&gt;$$a &amp;#x3D; (XX^T)^{-1}X^Ty$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;逻辑斯蒂回归&amp;#x2F;对数几率回归&lt;br&gt;线性回归一个明显的问题是对离群点导致模型建模不稳定，使结果有偏，为了缓解这个问题（特别是在二分类场景中）带来的影响，可考虑逻辑斯蒂回归&lt;br&gt;逻辑斯蒂回归就是在回归模型中引入 sigmoid函数的一种非线性回归模型&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;逻辑斯蒂回归-Logistic-Regression&#34;&gt;&lt;a href=&#34;#逻辑斯蒂回归-Logistic-Regression&#34; class=&#34;headerlink&#34; title=&#34;逻辑斯蒂回归 (Logistic Regression)&#34;&gt;&lt;/a&gt;逻辑斯蒂回归 (Logistic Regression)&lt;/h1&gt;&lt;p&gt;逻辑斯蒂回归（logistic regression）就是在回归模型中引入 sigmoid 函数的一种非线性回归模型。Logistic 回归模型可如下表示：&lt;/p&gt;
&lt;p&gt;$$ y &amp;#x3D; \frac{1}{1 + e^{-z}} &amp;#x3D; \frac{1}{1 + e^{-(w^T x + b)}} $$&lt;br&gt;其中 $y \in (0, 1)$，$z &amp;#x3D; w^T x + b$。&lt;br&gt;这里 $\frac{1}{1 + e^{-z}}$ 是 sigmoid 函数，$x \in \mathbb{R}^d$ 是输入数据，$w \in \mathbb{R}^d$ 和 $b \in \mathbb{R}$ 是回归函数的参数。&lt;/p&gt;
&lt;p&gt;逻辑斯蒂回归多用于&amp;#x3D;&amp;#x3D;二分类&amp;#x3D;&amp;#x3D;问题&lt;br&gt;Sigmoid 函数将任意实数映射到区间(0,1)，这正好符合“概率”的取值范围，所以函数的输出y可以被解释为输入数据x属于正例的概率&lt;br&gt;因此我们可以将输出 $ y $ 解释为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在给定输入特征 $ x $ 的条件下，该样本属于正类（例如类别 1）的概率。&lt;br&gt;即：&lt;br&gt;$$&lt;br&gt;y &amp;#x3D; P(y &amp;#x3D; 1 \mid x)&lt;br&gt;$$&lt;br&gt;如果 $P(y&amp;#x3D;1|x)$ 表示给定输入 $x$ 属于正类的概率，则 $1 - P(y&amp;#x3D;1|x)$ 表示属于负类的概率。&lt;br&gt;$\frac{P(y&amp;#x3D;1|x)}{1 - P(y&amp;#x3D;1|x)}$ 就是正类相对于负类的优势比。所以&amp;gt;1就归为正类，反之就是负类。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$&lt;br&gt;\log \frac{P(y&amp;#x3D;1|x)}{P(y&amp;#x3D;0|x)} &amp;#x3D; {w^T x + b} &amp;gt; \log{1} &amp;#x3D; 0&lt;br&gt;$$&lt;br&gt;从这里可以看出，logistic回归本质上是一个线性模型。在预测时，可以计算线性函数$w^T x + b$取值是否大于0来判断输入数据x的类别归属&lt;/p&gt;
&lt;p&gt;为了找到最优参数w和b，我们使用最大似然估计，假设每个样本独立同分布，则&lt;br&gt;……&lt;br&gt;公式懒得敲了，&lt;/p&gt;
&lt;p&gt;为什么基于相关性的方法可能会导致模型的不可解释性和不稳定性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因果特征和非因果特征&lt;/li&gt;
&lt;li&gt;Making V⊥Y: 最终目标是让非因果特征 V 与输出 Y 独立，即消除虚假相关性，使得模型更加稳定和可解释&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;决策树&#34;&gt;&lt;a href=&#34;#决策树&#34; class=&#34;headerlink&#34; title=&#34;决策树&#34;&gt;&lt;/a&gt;决策树&lt;/h1&gt;&lt;p&gt;决策树是一种通过树形结构来进行分类的方法&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://example.com/2025/06/11/AI/week4-5/</guid>
            <title>Week4-5</title>
            <link>http://example.com/2025/06/11/AI/week4-5/</link>
            <category>人工智能</category>
            <pubDate>Wed, 11 Jun 2025 00:00:00 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;#Ch3 搜索算法&lt;/p&gt;
&lt;h2 id=&#34;无信息搜索&#34;&gt;&lt;a href=&#34;#无信息搜索&#34; class=&#34;headerlink&#34; title=&#34;无信息搜索&#34;&gt;&lt;/a&gt;无信息搜索&lt;/h2&gt;&lt;p&gt;BFS DFS 略&lt;/p&gt;
&lt;h2 id=&#34;启发式搜索&#34;&gt;&lt;a href=&#34;#启发式搜索&#34; class=&#34;headerlink&#34; title=&#34;启发式搜索&#34;&gt;&lt;/a&gt;启发式搜索&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;贪婪优先搜索&lt;ul&gt;
&lt;li&gt;每次取最短的；缺点：不一定是最优的&lt;/li&gt;
&lt;li&gt;时间和空间复杂度均为 $O(b_m)$，b是搜索树分支因子，m是最大深度&lt;br&gt;&lt;img data-src=&#34;/figure1.png&#34;&gt;&lt;br&gt;:::info&lt;br&gt;每次取当前节点的下一个节点到终点中直线距离最短的&lt;br&gt;:::&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A*算法&lt;ul&gt;
&lt;li&gt;评价函数：f(n) &amp;#x3D; g(n) + h(n)&lt;/li&gt;
&lt;li&gt;代价函数 g(n) 表示从起始结点到结点n的开销代价值&lt;/li&gt;
&lt;li&gt;启发函数 h(n) 表示从结点n到目标结点路径中所估算的最小开销代价值。&lt;/li&gt;
&lt;li&gt;评价函数 f(n) 可视为经过结点n、具有最小开销代价值的路径。&lt;ul&gt;
&lt;li&gt;在最短路径问题中，g(?)为当前选择的路径的实际距离，即从上一个节点到下一个节点的实际距离，?(?)为下一个节点到目标城市的直线距离。每一次搜索，下一个节点选择与此刻城市连接的所有节点中，g(?)+?(?)最小的城市节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;:::info&lt;br&gt;取（当前节点到下一节点的距离+下一节点到目标城市的距离）最短的&lt;br&gt;:::&lt;br&gt;A*算法的完备性和最优性取决于搜索问题和启发函数的性质&lt;br&gt;一个良好的启发函数需要满足:可容性（admissible）;一致性（consistency）&lt;br&gt;如果启发函数是可容的，那么树搜索的A*算法满足最优性(最优性:搜索算法是否能保证找到的第一个解是最优解)&lt;br&gt;满足一致性条件的启发函数一定满足可容性条件，反之不一定&lt;/p&gt;
&lt;h2 id=&#34;对抗搜索&#34;&gt;&lt;a href=&#34;#对抗搜索&#34; class=&#34;headerlink&#34; title=&#34;对抗搜索&#34;&gt;&lt;/a&gt;对抗搜索&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;最小最大搜索（minimax）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最小最大搜索是一个在你和对手轮流行动的情况下，为你自己寻找最优策略的算法。&lt;/li&gt;
&lt;li&gt;算法：略&lt;/li&gt;
&lt;li&gt;时间复杂度：$O(b^m)$&lt;/li&gt;
&lt;li&gt;空间复杂度：$O(bm)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\alpha-\beta剪枝&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimax 会穷举整个博弈树，但我们可以用剪枝技巧跳过一些无用分支，让它跑得更快&lt;/li&gt;
&lt;li&gt;max层的下界取下一层（上界）里面最大的；min层的上界取下一层（下界）里面最小的&lt;br&gt;懒得写直接看例子：&lt;br&gt;&lt;img data-src=&#34;/figure2.png&#34;&gt;&lt;br&gt; Alpha-Beta 剪枝算法什么时候扩展的结点数量最少？&lt;/li&gt;
&lt;li&gt;每一层最左端结点的所有孩子结点均被访问，其他节点仅有最左端孩子结点被访问、其他孩子结点被剪枝。&lt;br&gt; 如果一个节点导致了其兄弟节点被剪枝，可知其孩子节点必然被扩展。&lt;/li&gt;
&lt;li&gt;最优效率下时间复杂度：$O(b^{m&amp;#x2F;2})$  (或者m+1);最差的就是完全没剪枝和minimax一样&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;蒙特卡洛树搜索&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择、扩展、模拟、反向传播&lt;/li&gt;
&lt;li&gt;悔值函数&lt;br&gt;:::info&lt;br&gt;没完全懂，后面再回来研究&lt;br&gt;:::&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://example.com/2025/05/20/AI/week1/</guid>
            <title>Week1</title>
            <link>http://example.com/2025/05/20/AI/week1/</link>
            <category>人工智能</category>
            <pubDate>Tue, 20 May 2025 00:00:00 +0800</pubDate>
            <description><![CDATA[ &lt;blockquote&gt;
&lt;p&gt;2025-2026春夏人工智能课程笔记&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;Ch1-绪论&#34;&gt;&lt;a href=&#34;#Ch1-绪论&#34; class=&#34;headerlink&#34; title=&#34;Ch1 绪论&#34;&gt;&lt;/a&gt;Ch1 绪论&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;人工智能求解：&lt;ul&gt;
&lt;li&gt;以符号主义为核心的逻辑推理：将概念（如命题等）符号化，从若干判断（前提）出发得到新判断（结论）&lt;/li&gt;
&lt;li&gt;以问题求解为核心的探寻搜索:探寻搜索依据已有信息来寻找满足约束条件的待求解问题的答案&lt;/li&gt;
&lt;li&gt;以数据驱动为核心的机器学习:从数据中发现数据所承载语义（如概念）的内在模式&lt;/li&gt;
&lt;li&gt;以行为主义为核心的强化学习:根据环境所提供的奖罚反馈来学习所处状态可施加的最佳行动，在“探索（未知空间）-利用（已有经验）（exploration vs. exploitation）”之间寻找平衡，完成某个序列化任务，具备自我学习能力&lt;/li&gt;
&lt;li&gt;以博弈对抗为核心的群体智能:从“数据拟合”优化解的求取向“均衡解”的求取迈进&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://example.com/2025/05/20/AI/week2-3/</guid>
            <title>Week2-3</title>
            <link>http://example.com/2025/05/20/AI/week2-3/</link>
            <category>人工智能</category>
            <pubDate>Tue, 20 May 2025 00:00:00 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;Ch2-知识表达与推理&#34;&gt;&lt;a href=&#34;#Ch2-知识表达与推理&#34; class=&#34;headerlink&#34; title=&#34;Ch2 知识表达与推理&#34;&gt;&lt;/a&gt;Ch2 知识表达与推理&lt;/h1&gt;&lt;h2 id=&#34;命题逻辑&#34;&gt;&lt;a href=&#34;#命题逻辑&#34; class=&#34;headerlink&#34; title=&#34;命题逻辑&#34;&gt;&lt;/a&gt;命题逻辑&lt;/h2&gt;&lt;p&gt;&lt;img data-src=&#34;/img1.png&#34;&gt;&lt;br&gt;真值表：&lt;br&gt;&lt;img data-src=&#34;/img2.png&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“条件”命题联结词中前提为假时命题结论永远为真，bi-conditional只有两个都是true或者都是false才是true&lt;br&gt;逻辑等价：给定命题p和命题q，如果&amp;#x3D;&amp;#x3D;p和q在所有情况下都具有同样真假结果&amp;#x3D;&amp;#x3D;，那么p和q在逻辑上等价，一般用 $\equiv$ 来表示，即p $\equiv$ q。&lt;br&gt;判断逻辑等价：画真值表&lt;br&gt;逻辑等价式：&lt;br&gt;&lt;img data-src=&#34;/img3.jpg&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img4.png&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;normal form&lt;ul&gt;
&lt;li&gt;有限个简单合取式构成的析取式称为析取(or)范式&lt;/li&gt;
&lt;li&gt;由有限个简单析取式构成的合取式称为合取(and)范式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;谓词逻辑&#34;&gt;&lt;a href=&#34;#谓词逻辑&#34; class=&#34;headerlink&#34; title=&#34;谓词逻辑&#34;&gt;&lt;/a&gt;谓词逻辑&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;全称量词与存在量词&lt;/li&gt;
&lt;li&gt;约束变元、自由变元&lt;br&gt;:::info&lt;br&gt;在约束变元相同的情况下，量词的运算满足分配律：全称量词对析取没有分配律、存在量词对合取没有分配律&lt;br&gt;:::&lt;br&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(A(x) \lor B(x)) \equiv (\forall x)A(x) \lor (\forall x)B(x) 不成立&lt;br&gt;\end{aligned}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(A(x) \land B(x)) \equiv (\forall x)A(x) \land (\forall x)B(x) 成立&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists x)(A(x) \lor B(x)) \equiv (\exists x)A(x) \lor (\exists x)B(x) 成立&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists x)(A(x) \land B(x)) \equiv (\exists x)A(x) \land (\exists x)B(x) 不成立&lt;br&gt;\end{aligned}$$&lt;br&gt;:::info&lt;br&gt;当公式中存在多个量词时，若多个量词都是全称量词或者都是存在量词，则量词的位置可以互换；若多个量词中既有全称量词又有存在量词，则量词的位置不可以随意互换&lt;br&gt;:::&lt;br&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(\forall y)A(x, y) \equiv (\forall y)(\forall x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists x)(\exists y)A(x, y) \equiv (\exists y)(\exists x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(\forall y)A(x, y) \equiv (\exists y)(\forall x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(\forall y)A(x, y) \equiv (\exists x)(\forall y)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists y)(\forall x)A(x, y) \equiv (\forall x)(\exists y)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists x)(\forall y)A(x, y) \equiv (\forall y)(\exists x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(\exists y)A(x, y) \equiv (\exists y)(\exists x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall y)(\exists x)A(x, y) \equiv (\exists x)(\exists y)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;利用谓词逻辑进行推理&lt;ul&gt;
&lt;li&gt;全称量词消去： $(\forall x) A(x) \equiv A(y)$&lt;/li&gt;
&lt;li&gt;全称量词引入： $A(y) \equiv (\forall x) A(x)$&lt;/li&gt;
&lt;li&gt;存在量词消去： $(\exists x) A(x) \equiv A(c)$&lt;/li&gt;
&lt;li&gt;存在量词引入： $A(c) \equiv (\exists x) A(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;知识图谱推理&#34;&gt;&lt;a href=&#34;#知识图谱推理&#34; class=&#34;headerlink&#34; title=&#34;知识图谱推理&#34;&gt;&lt;/a&gt;知识图谱推理&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;知识图谱可视为包含多种关系的图。在图中，每个节点是一个实体（如人名、地名、事件和活动等），任意两个节点之间的边表示这两个节点之间存在的关系。&lt;/li&gt;
&lt;li&gt;可将知识图谱中任意两个相连节点及其连接边表示成一个三元组（triplet）,即 (left_node, relation, right_node)&lt;br&gt;两类代表性方法：&lt;/li&gt;
&lt;li&gt;归纳逻辑程序设计 (inductive logic programming，ILP)算法&lt;/li&gt;
&lt;li&gt;路径排序算法（path ranking algorithm, PRA）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ILP: 一阶归纳学习FOIL（First Order Inductive Learner）&lt;br&gt;推理手段: 正例集合 + 反例集合 + 背景知识样例 ⟹ 目标谓词作为结论的推理规则&lt;br&gt;&lt;img data-src=&#34;/img5.png&#34;&gt;&lt;br&gt;懒得写了，看ppt吧&lt;br&gt;&lt;img data-src=&#34;/img6.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;PRA: 路径排序算法&lt;br&gt;&lt;img data-src=&#34;/img7.png&#34;&gt;&lt;br&gt;(4)的意思是看两个实体能不能通过(3)的关系从第一个走到第二个。&lt;br&gt;后面的1表示正例，-1表示负例。&lt;/p&gt;
&lt;h2 id&gt;&lt;a href=&#34;#&#34; class=&#34;headerlink&#34; title&gt;&lt;/a&gt;&lt;/h2&gt; ]]></description>
        </item>
    </channel>
</rss>
