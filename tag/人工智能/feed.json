{
    "version": "https://jsonfeed.org/version/1",
    "title": " • All posts by \"人工智能\" tag",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/06/12/AI/week6-7/",
            "url": "http://example.com/2025/06/12/AI/week6-7/",
            "title": "Week6-7",
            "date_published": "2025-06-11T16:00:00.000Z",
            "content_html": "<h1 id=\"Ch4-机器学习\"><a href=\"#Ch4-机器学习\" class=\"headerlink\" title=\"Ch4 机器学习\"></a>Ch4 机器学习</h1><h2 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h2><ul>\n<li>标注数据</li>\n<li>学习模型</li>\n<li>损失函数<br>典型的损失函数<br><img data-src=\"/figure2.png\"></li>\n</ul>\n<p>经验风险(empirical risk )</p>\n<ul>\n<li>训练集中数据产生的损失。</li>\n<li>经验风险越小说明学习模型对训练数据拟合程度越好。</li>\n</ul>\n<p>期望风险(expected risk):</p>\n<ul>\n<li>当测试集中存在无穷多数据时产生的损失。</li>\n<li>期望风险越小，学习所得模型越好。</li>\n</ul>\n<p>经验风险最小化</p>\n<p>$\\min_{f \\in \\Phi} \\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i))$</p>\n<p>期望风险最小化</p>\n<p>$\\min_{f \\in \\Phi} \\int_{x \\times y} Loss(y, f(x)) P(x, y) dx dy$</p>\n<p>模型泛化能力与经验风险、期望风险的关系</p>\n<table>\n<thead>\n<tr>\n<th>经验风险小（训练集上表现好）</th>\n<th>期望风险小（测试集上表现好）</th>\n<th>泛化能力强</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>经验风险小（训练集上表现好）</td>\n<td>期望风险大（测试集上表现不好）</td>\n<td>过学习（模型过于复杂）</td>\n</tr>\n<tr>\n<td>经验风险大（训练集上表现不好）</td>\n<td>期望风险大（测试集上表现不好）</td>\n<td>欠学习</td>\n</tr>\n<tr>\n<td>经验风险大（训练集上表现不好）</td>\n<td>期望风险小（测试集上表现好）</td>\n<td>“神仙算法”或“黄粱美梦”</td>\n</tr>\n</tbody></table>\n<p>结构风险最小化 (structural risk minimization)</p>\n<p>为了防止过拟合，在经验风险上加上表示模型复杂度的正则化项 (regularizer) 或惩罚项 (penalty term):</p>\n<p>$$\\min_{f \\in \\Phi} \\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i)) + \\lambda J(f)$$</p>\n<ul>\n<li>经验风险: $\\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i))$</li>\n<li>模型复杂度: $\\lambda J(f)$</li>\n</ul>\n<p>监督学习方法又可以分为 生成方法 (generative approach) 和 判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative model)和判别模型(discriminative model)<br><img data-src=\"/figure1.png\"></p>\n<h1 id=\"回归分析\"><a href=\"#回归分析\" class=\"headerlink\" title=\"回归分析\"></a>回归分析</h1><h2 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h2><ul>\n<li>一元线性回归</li>\n</ul>\n<p>$$y_i &#x3D; ax_i + b \\quad (1 \\leq i \\leq n)$$</p>\n<p>$$a &#x3D; \\frac{\\sum_{i&#x3D;1}^{n} x_i y_i - n \\bar{x} \\bar{y}}{\\sum_{i&#x3D;1}^{n} x_i^2 - n \\bar{x}^2}$$</p>\n<p>$$b &#x3D; \\bar{y} - a \\bar{x}$$</p>\n<ul>\n<li><p>多元线性回归<br>$$f(x_i) &#x3D; a_0 + \\sum_{j&#x3D;1}^{D} a_j x_{i,j} &#x3D; a_0 + \\mathbf{a}^T \\mathbf{x}_i$$<br>a是要求的参数，x是输入的数据，f是预测值。<br>为了方便，使用矩阵来表示所有的训练数据和数据标签。<br>$$X &#x3D; [x_1, …, x_m], \\quad y &#x3D; [y_1, …, y_m]$$<br>最小化均方误差得到：<br>$$a &#x3D; (XX^T)^{-1}X^Ty$$</p>\n</li>\n<li><p>逻辑斯蒂回归&#x2F;对数几率回归<br>线性回归一个明显的问题是对离群点导致模型建模不稳定，使结果有偏，为了缓解这个问题（特别是在二分类场景中）带来的影响，可考虑逻辑斯蒂回归<br>逻辑斯蒂回归就是在回归模型中引入 sigmoid函数的一种非线性回归模型</p>\n</li>\n</ul>\n<h1 id=\"逻辑斯蒂回归-Logistic-Regression\"><a href=\"#逻辑斯蒂回归-Logistic-Regression\" class=\"headerlink\" title=\"逻辑斯蒂回归 (Logistic Regression)\"></a>逻辑斯蒂回归 (Logistic Regression)</h1><p>逻辑斯蒂回归（logistic regression）就是在回归模型中引入 sigmoid 函数的一种非线性回归模型。Logistic 回归模型可如下表示：</p>\n<p>$$ y &#x3D; \\frac{1}{1 + e^{-z}} &#x3D; \\frac{1}{1 + e^{-(w^T x + b)}} $$<br>其中 $y \\in (0, 1)$，$z &#x3D; w^T x + b$。<br>这里 $\\frac{1}{1 + e^{-z}}$ 是 sigmoid 函数，$x \\in \\mathbb{R}^d$ 是输入数据，$w \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$ 是回归函数的参数。</p>\n<p>逻辑斯蒂回归多用于&#x3D;&#x3D;二分类&#x3D;&#x3D;问题<br>Sigmoid 函数将任意实数映射到区间(0,1)，这正好符合“概率”的取值范围，所以函数的输出y可以被解释为输入数据x属于正例的概率<br>因此我们可以将输出 $ y $ 解释为：</p>\n<blockquote>\n<p>在给定输入特征 $ x $ 的条件下，该样本属于正类（例如类别 1）的概率。<br>即：<br>$$<br>y &#x3D; P(y &#x3D; 1 \\mid x)<br>$$<br>如果 $P(y&#x3D;1|x)$ 表示给定输入 $x$ 属于正类的概率，则 $1 - P(y&#x3D;1|x)$ 表示属于负类的概率。<br>$\\frac{P(y&#x3D;1|x)}{1 - P(y&#x3D;1|x)}$ 就是正类相对于负类的优势比。所以&gt;1就归为正类，反之就是负类。</p>\n</blockquote>\n<p>$$<br>\\log \\frac{P(y&#x3D;1|x)}{P(y&#x3D;0|x)} &#x3D; {w^T x + b} &gt; \\log{1} &#x3D; 0<br>$$<br>从这里可以看出，logistic回归本质上是一个线性模型。在预测时，可以计算线性函数$w^T x + b$取值是否大于0来判断输入数据x的类别归属</p>\n<p>为了找到最优参数w和b，我们使用最大似然估计，假设每个样本独立同分布，则<br>……<br>公式懒得敲了，</p>\n<p>为什么基于相关性的方法可能会导致模型的不可解释性和不稳定性</p>\n<ul>\n<li>因果特征和非因果特征</li>\n<li>Making V⊥Y: 最终目标是让非因果特征 V 与输出 Y 独立，即消除虚假相关性，使得模型更加稳定和可解释</li>\n</ul>\n<h1 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h1><p>决策树是一种通过树形结构来进行分类的方法</p>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/06/11/AI/week4-5/",
            "url": "http://example.com/2025/06/11/AI/week4-5/",
            "title": "Week4-5",
            "date_published": "2025-06-10T16:00:00.000Z",
            "content_html": "<p>#Ch3 搜索算法</p>\n<h2 id=\"无信息搜索\"><a href=\"#无信息搜索\" class=\"headerlink\" title=\"无信息搜索\"></a>无信息搜索</h2><p>BFS DFS 略</p>\n<h2 id=\"启发式搜索\"><a href=\"#启发式搜索\" class=\"headerlink\" title=\"启发式搜索\"></a>启发式搜索</h2><ul>\n<li>贪婪优先搜索<ul>\n<li>每次取最短的；缺点：不一定是最优的</li>\n<li>时间和空间复杂度均为 $O(b_m)$，b是搜索树分支因子，m是最大深度<br><img data-src=\"/figure1.png\"><br>:::info<br>每次取当前节点的下一个节点到终点中直线距离最短的<br>:::</li>\n</ul>\n</li>\n<li>A*算法<ul>\n<li>评价函数：f(n) &#x3D; g(n) + h(n)</li>\n<li>代价函数 g(n) 表示从起始结点到结点n的开销代价值</li>\n<li>启发函数 h(n) 表示从结点n到目标结点路径中所估算的最小开销代价值。</li>\n<li>评价函数 f(n) 可视为经过结点n、具有最小开销代价值的路径。<ul>\n<li>在最短路径问题中，g(?)为当前选择的路径的实际距离，即从上一个节点到下一个节点的实际距离，?(?)为下一个节点到目标城市的直线距离。每一次搜索，下一个节点选择与此刻城市连接的所有节点中，g(?)+?(?)最小的城市节点。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>:::info<br>取（当前节点到下一节点的距离+下一节点到目标城市的距离）最短的<br>:::<br>A*算法的完备性和最优性取决于搜索问题和启发函数的性质<br>一个良好的启发函数需要满足:可容性（admissible）;一致性（consistency）<br>如果启发函数是可容的，那么树搜索的A*算法满足最优性(最优性:搜索算法是否能保证找到的第一个解是最优解)<br>满足一致性条件的启发函数一定满足可容性条件，反之不一定</p>\n<h2 id=\"对抗搜索\"><a href=\"#对抗搜索\" class=\"headerlink\" title=\"对抗搜索\"></a>对抗搜索</h2><ul>\n<li><p>最小最大搜索（minimax）</p>\n<ul>\n<li>最小最大搜索是一个在你和对手轮流行动的情况下，为你自己寻找最优策略的算法。</li>\n<li>算法：略</li>\n<li>时间复杂度：$O(b^m)$</li>\n<li>空间复杂度：$O(bm)$</li>\n</ul>\n</li>\n<li><p>\\alpha-\\beta剪枝</p>\n<ul>\n<li>Minimax 会穷举整个博弈树，但我们可以用剪枝技巧跳过一些无用分支，让它跑得更快</li>\n<li>max层的下界取下一层（上界）里面最大的；min层的上界取下一层（下界）里面最小的<br>懒得写直接看例子：<br><img data-src=\"/figure2.png\"><br> Alpha-Beta 剪枝算法什么时候扩展的结点数量最少？</li>\n<li>每一层最左端结点的所有孩子结点均被访问，其他节点仅有最左端孩子结点被访问、其他孩子结点被剪枝。<br> 如果一个节点导致了其兄弟节点被剪枝，可知其孩子节点必然被扩展。</li>\n<li>最优效率下时间复杂度：$O(b^{m&#x2F;2})$  (或者m+1);最差的就是完全没剪枝和minimax一样</li>\n</ul>\n</li>\n<li><p>蒙特卡洛树搜索</p>\n<ul>\n<li>选择、扩展、模拟、反向传播</li>\n<li>悔值函数<br>:::info<br>没完全懂，后面再回来研究<br>:::</li>\n</ul>\n</li>\n</ul>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/05/20/AI/week1/",
            "url": "http://example.com/2025/05/20/AI/week1/",
            "title": "Week1",
            "date_published": "2025-05-19T16:00:00.000Z",
            "content_html": "<blockquote>\n<p>2025-2026春夏人工智能课程笔记</p>\n</blockquote>\n<h1 id=\"Ch1-绪论\"><a href=\"#Ch1-绪论\" class=\"headerlink\" title=\"Ch1 绪论\"></a>Ch1 绪论</h1><ul>\n<li>人工智能求解：<ul>\n<li>以符号主义为核心的逻辑推理：将概念（如命题等）符号化，从若干判断（前提）出发得到新判断（结论）</li>\n<li>以问题求解为核心的探寻搜索:探寻搜索依据已有信息来寻找满足约束条件的待求解问题的答案</li>\n<li>以数据驱动为核心的机器学习:从数据中发现数据所承载语义（如概念）的内在模式</li>\n<li>以行为主义为核心的强化学习:根据环境所提供的奖罚反馈来学习所处状态可施加的最佳行动，在“探索（未知空间）-利用（已有经验）（exploration vs. exploitation）”之间寻找平衡，完成某个序列化任务，具备自我学习能力</li>\n<li>以博弈对抗为核心的群体智能:从“数据拟合”优化解的求取向“均衡解”的求取迈进</li>\n</ul>\n</li>\n</ul>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/05/20/AI/week2-3/",
            "url": "http://example.com/2025/05/20/AI/week2-3/",
            "title": "Week2-3",
            "date_published": "2025-05-19T16:00:00.000Z",
            "content_html": "<h1 id=\"Ch2-知识表达与推理\"><a href=\"#Ch2-知识表达与推理\" class=\"headerlink\" title=\"Ch2 知识表达与推理\"></a>Ch2 知识表达与推理</h1><h2 id=\"命题逻辑\"><a href=\"#命题逻辑\" class=\"headerlink\" title=\"命题逻辑\"></a>命题逻辑</h2><p><img data-src=\"/img1.png\"><br>真值表：<br><img data-src=\"/img2.png\"></p>\n<blockquote>\n<p>“条件”命题联结词中前提为假时命题结论永远为真，bi-conditional只有两个都是true或者都是false才是true<br>逻辑等价：给定命题p和命题q，如果&#x3D;&#x3D;p和q在所有情况下都具有同样真假结果&#x3D;&#x3D;，那么p和q在逻辑上等价，一般用 $\\equiv$ 来表示，即p $\\equiv$ q。<br>判断逻辑等价：画真值表<br>逻辑等价式：<br><img data-src=\"/img3.jpg\"><br><img data-src=\"/img4.png\"></p>\n</blockquote>\n<ul>\n<li>normal form<ul>\n<li>有限个简单合取式构成的析取式称为析取(or)范式</li>\n<li>由有限个简单析取式构成的合取式称为合取(and)范式</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"谓词逻辑\"><a href=\"#谓词逻辑\" class=\"headerlink\" title=\"谓词逻辑\"></a>谓词逻辑</h2><ul>\n<li>全称量词与存在量词</li>\n<li>约束变元、自由变元<br>:::info<br>在约束变元相同的情况下，量词的运算满足分配律：全称量词对析取没有分配律、存在量词对合取没有分配律<br>:::<br>$$\\begin{aligned}<br>(\\forall x)(A(x) \\lor B(x)) \\equiv (\\forall x)A(x) \\lor (\\forall x)B(x) 不成立<br>\\end{aligned}$$</li>\n</ul>\n<p>$$\\begin{aligned}<br>(\\forall x)(A(x) \\land B(x)) \\equiv (\\forall x)A(x) \\land (\\forall x)B(x) 成立<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(A(x) \\lor B(x)) \\equiv (\\exists x)A(x) \\lor (\\exists x)B(x) 成立<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(A(x) \\land B(x)) \\equiv (\\exists x)A(x) \\land (\\exists x)B(x) 不成立<br>\\end{aligned}$$<br>:::info<br>当公式中存在多个量词时，若多个量词都是全称量词或者都是存在量词，则量词的位置可以互换；若多个量词中既有全称量词又有存在量词，则量词的位置不可以随意互换<br>:::<br>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\forall y)(\\forall x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(\\exists y)A(x, y) \\equiv (\\exists y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\exists y)(\\forall x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\exists x)(\\forall y)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists y)(\\forall x)A(x, y) \\equiv (\\forall x)(\\exists y)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(\\forall y)A(x, y) \\equiv (\\forall y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\exists y)A(x, y) \\equiv (\\exists y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall y)(\\exists x)A(x, y) \\equiv (\\exists x)(\\exists y)A(x, y)<br>\\end{aligned}$$</p>\n<ul>\n<li>利用谓词逻辑进行推理<ul>\n<li>全称量词消去： $(\\forall x) A(x) \\equiv A(y)$</li>\n<li>全称量词引入： $A(y) \\equiv (\\forall x) A(x)$</li>\n<li>存在量词消去： $(\\exists x) A(x) \\equiv A(c)$</li>\n<li>存在量词引入： $A(c) \\equiv (\\exists x) A(x)$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"知识图谱推理\"><a href=\"#知识图谱推理\" class=\"headerlink\" title=\"知识图谱推理\"></a>知识图谱推理</h2><ul>\n<li>知识图谱可视为包含多种关系的图。在图中，每个节点是一个实体（如人名、地名、事件和活动等），任意两个节点之间的边表示这两个节点之间存在的关系。</li>\n<li>可将知识图谱中任意两个相连节点及其连接边表示成一个三元组（triplet）,即 (left_node, relation, right_node)<br>两类代表性方法：</li>\n<li>归纳逻辑程序设计 (inductive logic programming，ILP)算法</li>\n<li>路径排序算法（path ranking algorithm, PRA）</li>\n</ul>\n<p>ILP: 一阶归纳学习FOIL（First Order Inductive Learner）<br>推理手段: 正例集合 + 反例集合 + 背景知识样例 ⟹ 目标谓词作为结论的推理规则<br><img data-src=\"/img5.png\"><br>懒得写了，看ppt吧<br><img data-src=\"/img6.png\"></p>\n<p>PRA: 路径排序算法<br><img data-src=\"/img7.png\"><br>(4)的意思是看两个实体能不能通过(3)的关系从第一个走到第二个。<br>后面的1表示正例，-1表示负例。</p>\n<h2 id><a href=\"#\" class=\"headerlink\" title></a></h2>",
            "tags": [
                "人工智能"
            ]
        }
    ]
}