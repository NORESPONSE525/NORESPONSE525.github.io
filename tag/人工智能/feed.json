{
    "version": "https://jsonfeed.org/version/1",
    "title": "NoResponse's Blog • All posts by \"人工智能\" tag",
    "description": "成分复杂的CSer from ZJU",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2025/06/16/AI/week10-11/",
            "url": "http://example.com/2025/06/16/AI/week10-11/",
            "title": "Week10-11",
            "date_published": "2025-06-15T16:00:00.000Z",
            "content_html": "<h1 id=\"深度学习\"><a href=\"#深度学习\" class=\"headerlink\" title=\"深度学习\"></a>深度学习</h1><h2 id=\"前馈神经网络\"><a href=\"#前馈神经网络\" class=\"headerlink\" title=\"前馈神经网络\"></a>前馈神经网络</h2><ul>\n<li>神经元</li>\n<li>感知机（多一个激活函数）</li>\n<li>激活函数: .RelU, Sigmoid, Softmax, tanh..</li>\n<li>损失函数: MSE, Cross Entropy…s</li>\n<li>参数优化: BP, 梯度下降<br>具体懒得写了，都说烂了<br>记一下这几个激活函数的形状和应用: tanh和sigmoid大多用于二分类，RelU一般用在隐藏层，Softmax用在多分类而且概率和为1</li>\n</ul>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><ul>\n<li><p>了解卷积操作和操作之后的结果</p>\n</li>\n<li><p>池化操作（最大池化、平均池化）<br>卷积层负责提取图像中的局部特征；<br>池化层用来大幅降低参数量级(降维)；<br>激活函数负责非线性化；<br>全连接层类似传统神经网络的部分，用来输出想要的结果</p>\n</li>\n<li><p>神经网络正则化：为了缓解神经网络在训练过程中出现的过拟合问题，需要采取一些正则化技术来提升神经网络的泛化能力(generalization)</p>\n<ul>\n<li>Dropout：在训练神经网络过程中，每次参数更新时随机丢掉一部分神经元来减少神经网络复杂度，防止过拟合<br> <img data-src=\"/f1.jpg\"></li>\n<li>Batch-Normalization（批归一化）:通过规范化的手段，把神经网络每层中任意神经元的输入值分布改变到均值为0、方差为1的标准正态分布。防止梯度消失，收敛更快</li>\n<li>L1-Norm &amp; L2-Norm</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"循环神经网络（RNN）\"><a href=\"#循环神经网络（RNN）\" class=\"headerlink\" title=\"循环神经网络（RNN）\"></a>循环神经网络（RNN）</h2><p>RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息</p>\n<p>为了解决&#x3D;&#x3D;梯度消失问题&#x3D;&#x3D;，长短时记忆模型（Long Short-Term Memory，LSTM）被提出<br>LSTM：<br>与简单的循环神经网络结构不同，长短时记忆网络（Long Short-Term Memory，LSTM）中引入了&#x3D;&#x3D;内部记忆单元&#x3D;&#x3D;（internal memory cell）和&#x3D;&#x3D;门&#x3D;&#x3D;（gates）两种结构来对当前时刻输入信息以及前序时刻所生成信息进行整合和传递。</p>\n<ul>\n<li>输入门(input gate)、遗忘门(forget gate)和输出门(output gate)三种gate</li>\n</ul>\n<p>门控循环单元（GRU）是一种对LSTM简化的深度学习模型。与长短时记忆网络相比，GRU不再使用记忆单元来传递信息，仅使用隐藏状态来进行信息的传递。相比于长短时记忆网络来说，GRU有更高的计算速度。</p>\n<h3 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h3><p>注意力机制对不同信息的关注程度（重要程度）由权值来体现，注意力机制可以视为&#x3D;&#x3D;查询矩阵(Query)&#x3D;&#x3D;&#x3D;&#x3D;、键(key)&#x3D;&#x3D;以及&#x3D;&#x3D;加权平均值&#x3D;&#x3D;构成了多层感知机(Multilayer Perceptron, MLP)</p>\n<h2 id=\"深度生成学习（deep-generative-learning-model）\"><a href=\"#深度生成学习（deep-generative-learning-model）\" class=\"headerlink\" title=\"深度生成学习（deep generative learning model）\"></a>深度生成学习（deep generative learning model）</h2><p>判别模型vs生成模型<br>变分自编码器 (variational auto-encoder, VAE) 、 自回归模型 (Autoregressivemodels)与生成对抗网络（generative adversarial network，GAN）等</p>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/06/16/AI/week12-13/",
            "url": "http://example.com/2025/06/16/AI/week12-13/",
            "title": "Week12-13",
            "date_published": "2025-06-15T16:00:00.000Z",
            "content_html": "<h1 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h1><p>根据环境所提供的奖罚反馈来学习所处状态可施加的最佳行动，在“探索（未知空间）-利用（已有经验）（exploration vs. exploitation）”之间寻找平衡，完成某个序列化任务，具备自我学习能力</p>\n<ul>\n<li>智能体（agent）：智能体是强化学习算法的主体，它能够根据经验做出主观判断并执行动作，是整个智能系统的核心。</li>\n<li>环境（environment）：智能体以外的一切统称为环境，环境在与智能体的交互中，能被智能体所采取的动作影响，同时环境也能向智能体反馈状态和奖励。虽说智能体以外的一切都可视为环境，但在设计算法时常常会排除不相关的因素建立一个理想的环境模型来对算法功能进行模拟。</li>\n<li>状态（state）：状态可以理解为智能体对环境的一种理解和编码，通常包含了对智能体所采取决策产生影响的信息。</li>\n<li>动作（action）：动作是智能体对环境产生影响的方式，这里说的动作常常指概念上的动作，如果是在设计机器人时还需考虑动作的执行机构。</li>\n<li>策略（policy）：策略是智能体在所处状态下去执行某个动作的依据，即给定一个状态，智能体可根据一个策略来选择应该采取的动作。</li>\n<li>奖励（reward）：奖励是智能体序贯式采取一系列动作后从环境获得的收益。注意奖励概念是现实中奖励和惩罚的统合，一般用正值来代表实际奖励，用负值来代表实际惩罚。<br><img data-src=\"/f1.jpg\"></li>\n</ul>\n<p>强化学习的特点</p>\n<ul>\n<li>&#x3D;&#x3D;基于评估&#x3D;&#x3D;：强化学习利用环境评估当前策略，以此为依据进行优化</li>\n<li>&#x3D;&#x3D;交互性&#x3D;&#x3D;：强化学习的数据在与环境的交互中产生</li>\n<li>&#x3D;&#x3D;序列决策过程&#x3D;&#x3D;：智能主体在与环境的交互中需要作出一系列的决策，这些决策往往是前后关联的</li>\n</ul>\n<h2 id=\"离散马尔可夫过程（Discrete-Markov-Process）\"><a href=\"#离散马尔可夫过程（Discrete-Markov-Process）\" class=\"headerlink\" title=\"离散马尔可夫过程（Discrete Markov Process）\"></a>离散马尔可夫过程（Discrete Markov Process）</h2><p>满足马尔可夫性的离散随机过程<br>用数学归纳法，推出t+1时刻状态仅与t时刻状态有关（一阶马尔可夫链）<br>强化学习是一种机器学习方法，通过与环境的交互来学习最优策略。下面我将通过具体的例子来解释这些公式和概念。</p>\n<h3 id=\"马尔可夫奖励过程\"><a href=\"#马尔可夫奖励过程\" class=\"headerlink\" title=\"马尔可夫奖励过程\"></a>马尔可夫奖励过程</h3><p>$$<br>G_t &#x3D; R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots<br>$$<br>这个公式表示从时间步 t开始的累积回报 $G_t$，其中 $R_{t+i}$ 是在时间步 $t+i$ 获得的即时奖励，$\\gamma$ 是折扣因子（范围在 [0, 1] 之间），用于减少未来奖励对当前决策的影响。</p>\n<p>例: 假在一个迷宫中寻找宝藏，每走一步都有可能获得或失去一些分数（奖励）。如果你在第 1 步获得 10 分，在第 2 步获得 5 分，在第 3 步获得 2 分，且折扣因子 $\\gamma &#x3D; 0.9$，那么从第 0 步开始的累积回报 $G_0$ 就是：<br>$$<br>G_0 &#x3D; 10 + 0.9 \\times 5 + 0.9^2 \\times 2 &#x3D; 10 + 4.5 + 1.62 &#x3D; 16.12<br>$$</p>\n<p>强化学习的求解方法：</p>\n<ul>\n<li>基于价值（1.策略优化 2.策略评估）</li>\n<li>基于策略</li>\n<li>基于模型</li>\n</ul>\n<h3 id=\"策略学习\"><a href=\"#策略学习\" class=\"headerlink\" title=\"策略学习\"></a>策略学习</h3><p>价值函数 (value function):<br>$$<br>V_\\pi(s) &#x3D; E_\\pi[G_t | S_t &#x3D; s]<br>$$</p>\n<p>在状态 s 下，按照策略 $\\pi$ 行动后在未来所获得的回报的期望值。</p>\n<p>动作-价值函数 (action-value function):<br>$$<br>q_\\pi(s, a) &#x3D; E_\\pi[G_t | S_t &#x3D; s, A_t &#x3D; a]<br>$$</p>\n<p>在状态 s 下采取动作 a，按照策略 $\\pi 行动后在未来获得的回报值。</p>\n<p>例: 继续迷宫的例子，假设在某个位置（状态 s有两个选择：向左走（动作 $a_1$ 或向右走（动作 $a_2$）。根据历史数据，向左走的平均回报是 8 分，向右走的平均回报是 12 分。那么在这个状态下，动作-价值函数 $q_\\pi(s, a_1) &#x3D; 8$，$q_\\pi(s, a_2) &#x3D; 12$。</p>\n<h3 id=\"贝尔曼方程\"><a href=\"#贝尔曼方程\" class=\"headerlink\" title=\"贝尔曼方程\"></a>贝尔曼方程</h3><p>价值函数的贝尔曼方程:<br>$$<br>V_\\pi(s) &#x3D; \\sum_a \\pi(s, a) q_\\pi(s, a)<br>$$<br>在状态 s 下，采取各个动作的概率加权平均的动作-价值函数。</p>\n<p>动作-价值函数的贝尔曼方程:<br>$$<br>q_\\pi(s, a) &#x3D; \\sum_{s’} P(s’ | s, a) [R(s, a, s’) + \\gamma V_\\pi(s’)]<br>$$<br>在状态s采取动作a的概率*（采取a进入s’得到的回报+处于s’可以得到的回报）。</p>\n<h3 id=\"策略评估算法步骤\"><a href=\"#策略评估算法步骤\" class=\"headerlink\" title=\"策略评估算法步骤\"></a>策略评估算法步骤</h3><h4 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h4><ol>\n<li>初始化 $V_\\pi$ 函数：首先需要对所有状态 s 的价值函数 $V_\\pi(s)$ 进行初始化，通常可以设置为0或者任意合理的初始值。</li>\n<li>循环迭代：<ul>\n<li>对于每一个状态 $s \\in S$，根据当前的策略 $\\pi$ 和已知的状态转移概率 $Pr(s’|s, a)$，以及即时奖励 $R(s, a, s’)$，更新该状态的价值函数 $V_\\pi(s)$。</li>\n<li>更新公式如下：<br>$$<br>V_\\pi(s) \\leftarrow \\sum_{a \\in A} \\pi(a|s) \\sum_{s’ \\in S} Pr(s’|s, a) [R(s, a, s’) + \\gamma V_\\pi(s’)]<br>$$</li>\n</ul>\n</li>\n<li>直到收敛：重复上述迭代过程，直到价值函数 $V_\\pi(s)$ 在所有状态上都几乎不再发生变化，即达到收敛状态。这表明我们已经找到了在当前策略 $\\pi$ 下各个状态的稳定价值。<br>&#x3D;&#x3D;当前状态的价值是由后续状态的价值通过贝尔曼方程传播回来的&#x3D;&#x3D;</li>\n</ol>\n<p>策略评估：动态规划、蒙特卡洛采样、时序差分</p>\n<h4 id=\"蒙特卡洛采样\"><a href=\"#蒙特卡洛采样\" class=\"headerlink\" title=\"蒙特卡洛采样\"></a>蒙特卡洛采样</h4><p>通过随机采样来估计期望值</p>\n<ol>\n<li>选择起始状态：从状态空间 S 中选择不同的起始状态。</li>\n<li>按照当前策略采样轨迹：从每个起始状态出发，按照当前策略 $\\pi$ 生成若干条完整的轨迹（episode）。这些轨迹构成了集合 D。</li>\n<li>计算反馈：对于每条轨迹中的每个状态 s，记录该状态出现时对应的回报 $G_i$ 。</li>\n<li>平均回报：对所有轨迹中状态 s 出现时的回报进行平均，得到该状态的价值 $V_\\pi(s)$。<br>假设我们有 k 条轨迹，每条轨迹中状态 s 出现时对应的回报分别为$G_1, G_2, \\ldots, G_k$，那么该状态的价值 $V_\\pi(s)$ 可以通过以下公式计算：<br>$$<br>V_\\pi(s) &#x3D; \\frac{1}{k} \\sum_{i&#x3D;1}^k G_i<br>$$</li>\n</ol>\n<h4 id=\"时序差分-TD\"><a href=\"#时序差分-TD\" class=\"headerlink\" title=\"时序差分(TD)\"></a>时序差分(TD)</h4><p>时序差分算法通过在每个时间步 t 上更新状态价值函数 $V_\\pi(s_t)$，而不是在每个状态 $s_t$ 上进行批量更新。<br>$$<br>V_\\pi(s) \\leftarrow V_\\pi(s) + \\alpha [R(s, a, s’) + \\gamma V_\\pi(s’) - V_\\pi(s)]<br>$$<br>其中：</p>\n<ul>\n<li>$\\alpha$ 是学习率，通常取较小的值，如0.1。</li>\n<li>$R(s, a, s’)$ 是即时奖励</li>\n<li>$V_\\pi(s’)$ 是状态 $s’$ 的价值函数</li>\n<li>$\\gamma$ 是折扣因子，通常取较小的值，如0.9。</li>\n</ul>\n<h4 id=\"Q-learning\"><a href=\"#Q-learning\" class=\"headerlink\" title=\"Q-learning\"></a>Q-learning</h4><h4 id=\"DQN\"><a href=\"#DQN\" class=\"headerlink\" title=\"DQN\"></a>DQN</h4>",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/06/16/AI/week14-15/",
            "url": "http://example.com/2025/06/16/AI/week14-15/",
            "title": "Week14-15",
            "date_published": "2025-06-15T16:00:00.000Z",
            "content_html": "<h1 id=\"人工智能博弈\"><a href=\"#人工智能博弈\" class=\"headerlink\" title=\"人工智能博弈\"></a>人工智能博弈</h1><h2 id=\"博弈论\"><a href=\"#博弈论\" class=\"headerlink\" title=\"博弈论\"></a>博弈论</h2><p>博弈的要素：</p>\n<ul>\n<li>player</li>\n<li>strategy</li>\n<li>payoff</li>\n<li>rule</li>\n</ul>\n<h2 id=\"博弈策略求解\"><a href=\"#博弈策略求解\" class=\"headerlink\" title=\"博弈策略求解\"></a>博弈策略求解</h2><h3 id=\"遗憾最小化算法（Regret-Minimization）\"><a href=\"#遗憾最小化算法（Regret-Minimization）\" class=\"headerlink\" title=\"遗憾最小化算法（Regret Minimization）\"></a>遗憾最小化算法（Regret Minimization）</h3><p>下一步选择策略$\\Sigma_i$的概率P:<br>$$<br>P(\\sigma_i^{T+1}) &#x3D; \\begin{cases}<br>\\frac{\\text{Regret}<em>i^{T,+}(\\sigma_i)}{\\sum</em>{\\sigma_i’ \\in \\Sigma_i} \\text{Regret}<em>i^{T,+}(\\sigma_i’)} &amp; \\text{if } \\sum</em>{\\sigma_i’ \\in \\Sigma_i} \\text{Regret}_i^{T,+}(\\sigma_i’) &gt; 0 \\<br>\\frac{1}{|\\Sigma_i|} &amp; \\text{otherwise}<br>\\end{cases}<br>$$<br>为什么不直接选遗憾最大的：防止对手发现自己所采取的策略<br>+++info 例子</p>\n<ul>\n<li>假设两个玩家A和B进行石头-剪刀-布（Rock-Paper-Scissors, RPS）的游戏，获胜玩家收益为1分，失败玩家收益为-1分，平局则两个玩家收益均为零分。</li>\n<li>第一局时，若玩家A出石头（R），玩家B出布（P），则此时玩家A的收益 $\\mu_A(R, P) &#x3D; -1$，玩家B的收益为 $\\mu_B(P, R) &#x3D; 1$。</li>\n<li>对于玩家A来说，在玩家B出布（P）这个策略情况下，如果玩家A选择出布（P）或者剪刀（S），则玩家A对应的收益值 $\\mu_A(P, P) &#x3D; 0$ 或者 $\\mu_A(S, P) &#x3D; 1$。</li>\n<li>所以第一局之后，玩家A没有出布的遗憾值为 $\\mu_A(P, P) - \\mu_A(R, P) &#x3D; 0 - (-1) &#x3D; 1$，没有出剪刀的遗憾值为 $\\mu_A(S, P) - \\mu_A(R, P) &#x3D; 1 - (-1) &#x3D; 2$。</li>\n<li>所以在第二局中，玩家A选择石头、剪刀和布这三个策略的概率分别为 0、$\\frac{2}{3}$、$\\frac{1}{3}$。因此，玩家A趋向于在第二局中选择出剪刀这个策略。</li>\n<li>第二局中，玩家A选择剪刀和玩家B选择石头情况下，第二轮石头、剪刀、布的Regret分别为1，0，2，把前两轮的regret加起来计算概率，得到出石头、剪刀、布的概率分别为$\\frac{1}{6}$、$\\frac{2}{6}$、$\\frac{3}{6}$。<br>+++</li>\n</ul>\n<h3 id=\"双边匹配算法\"><a href=\"#双边匹配算法\" class=\"headerlink\" title=\"双边匹配算法\"></a>双边匹配算法</h3><p><img data-src=\"/f2.jpg\"><br>在第一轮中，4名男性分别向自己最喜欢的女性表白，而收到3人表白的女性A选择了自己最喜欢的男性3，另一个收到表白的女性B选择了男性4；在第二轮中，尚未匹配的男性1和男性2继续向自己第二喜欢的对象表白，收到表白的女性B选择了自己更喜欢的男性2而放弃了男性4；同理，继续三轮表白和选择，所有人都找到了自己的伴侣，且所有匹配都是稳定的。可以看出，使用G-S算法得到了稳定匹配的结果。</p>\n<h3 id=\"单边匹配算法-最大交易圈\"><a href=\"#单边匹配算法-最大交易圈\" class=\"headerlink\" title=\"单边匹配算法-最大交易圈\"></a>单边匹配算法-最大交易圈</h3><p><img data-src=\"/f3.jpg\"></p>\n<ol>\n<li>每个人指向最喜欢的物，每个物指向占有它的人</li>\n<li>如果有圈，就把打成交易的人和物和相关边都删掉</li>\n<li>继续<br><img data-src=\"/f4.jpg\"></li>\n</ol>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/06/15/AI/week8-9/",
            "url": "http://example.com/2025/06/15/AI/week8-9/",
            "title": "Week8-9",
            "date_published": "2025-06-14T16:00:00.000Z",
            "content_html": "<h1 id=\"K-Means\"><a href=\"#K-Means\" class=\"headerlink\" title=\"K-Means\"></a>K-Means</h1><p>问题描述：如何将n个数据依据其相似度大小将它们分别聚类到k个集合，使得每个数据仅属于一个聚类集合。</p>\n<ul>\n<li>初始化质心：随机选择k个数据点作为初始质心$c_1, c_2, …, c_k$。</li>\n<li>分配数据点：对于每个数据点$x_i$，计算它与所有质心的距离，并将其分配到距离最近的质心所在的簇中</li>\n<li>更新质心：对于每个簇，计算该簇内所有数据点的平均值，将该平均值作为新的质心。</li>\n<li>迭代过程：重复执行分配和更新步骤，直到质心不再发生变化或达到预设的最大迭代次数。</li>\n</ul>\n<h1 id=\"主成分分析-PCA\"><a href=\"#主成分分析-PCA\" class=\"headerlink\" title=\"主成分分析(PCA)\"></a>主成分分析(PCA)</h1><ul>\n<li>输入：n个d维样本数据所构成的矩阵$\\mathbf{X}$，降维后的维数l</li>\n<li>输出：映射矩阵$\\mathbf{W} &#x3D; {\\mathbf{w}_1, \\mathbf{w}_2, …, \\mathbf{w}_l}$<br>算法步骤：</li>\n</ul>\n<ol>\n<li><p>对于每个样本数据$\\mathbf{x}_i$进行中心化处理：<br>$$<br>\\mathbf{x}_i’ &#x3D; \\mathbf{x}<em>i - \\mu, \\quad \\mu &#x3D; \\frac{1}{n}\\sum</em>{j&#x3D;1}^{n} \\mathbf{x}_j<br>$$</p>\n</li>\n<li><p>计算原始样本数据的协方差矩阵：<br>$$<br>\\Sigma &#x3D; \\frac{1}{n-1} \\mathbf{X}^T \\mathbf{X}<br>$$</p>\n</li>\n<li><p>对协方差矩阵$\\Sigma$进行特征值分解，对所得特征根按其值大到小排序$\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_d$</p>\n</li>\n<li><p>取前$l$个最大特征根所对应特征向量$\\mathbf{w}_1, \\mathbf{w}_2, …, \\mathbf{w}_l$组成映射矩阵$\\mathbf{W}$</p>\n</li>\n<li><p>将每个样本数据$\\mathbf{x}<em>i$按照如下方法降维：<br>$$<br>(\\mathbf{x}<em>i)</em>{1 \\times d} (\\mathbf{W})</em>{d \\times l} &#x3D; 1 \\times l<br>$$<br>区分：</p>\n<table>\n<thead>\n<tr>\n<th>维度</th>\n<th>PCA</th>\n<th>LDA</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>类型</strong></td>\n<td>无监督</td>\n<td>有监督</td>\n</tr>\n<tr>\n<td><strong>目标</strong></td>\n<td>最大化方差，保留主要分布信息</td>\n<td>最大化类间距离，最小化类内距离</td>\n</tr>\n<tr>\n<td><strong>是否使用类别信息</strong></td>\n<td>? 不使用</td>\n<td>? 使用</td>\n</tr>\n<tr>\n<td><strong>适用任务</strong></td>\n<td>数据压缩、可视化、去噪</td>\n<td>分类任务的特征提取</td>\n</tr>\n<tr>\n<td><strong>降维后维度上限</strong></td>\n<td>可任意，但一般小于原维度</td>\n<td>最多降到 $k-1$ 维（$k$ 是类别数）</td>\n</tr>\n<tr>\n<td><strong>数学基础</strong></td>\n<td>协方差矩阵的特征值分解</td>\n<td>类间&#x2F;类内散度矩阵的广义特征值分解</td>\n</tr>\n</tbody></table>\n</li>\n</ol>\n<ul>\n<li>其他降维方法：<ul>\n<li>非负矩阵分解 （non-negative matrix factorization, NMF）</li>\n<li>多维尺度法（Metric multidimensional scaling, MDS）</li>\n<li>局部线性嵌入（Locally Linear Embedding，LLE）</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"特征人脸方法\"><a href=\"#特征人脸方法\" class=\"headerlink\" title=\"特征人脸方法\"></a>特征人脸方法</h1><p>输入时将每幅人脸图像转换成列向量<br><img data-src=\"/f1.jpg\"><br>算法描述</p>\n<ul>\n<li>输入：$n$个1024维人脸样本数据所构成的矩阵$\\mathbf{X}$，降维后的维数$l$</li>\n<li>输出：映射矩阵$\\mathbf{W} &#x3D; {\\mathbf{w}_1, \\mathbf{w}_2, …, \\mathbf{w}_l}$（其中每个$\\mathbf{w}_j (1 \\leq j \\leq l)$是一个特征人脸）<br>算法步骤</li>\n</ul>\n<ol>\n<li><p>中心化处理：</p>\n<ul>\n<li>对每个人脸样本数据$x_i$进行中心化处理：<br>$$<br>x_i’ &#x3D; x_i - \\mu, \\quad \\mu &#x3D; \\frac{1}{n}\\sum_{j&#x3D;1}^{n} x_j<br>$$</li>\n</ul>\n</li>\n<li><p>计算协方差矩阵：</p>\n<ul>\n<li>计算原始人脸样本数据的协方差矩阵：<br>$$<br>\\Sigma &#x3D; \\frac{1}{n-1} \\mathbf{X}^T \\mathbf{X}<br>$$</li>\n</ul>\n</li>\n<li><p>特征值分解：</p>\n<ul>\n<li>对协方差矩阵$\\Sigma$进行特征值分解，对所得特征根按从大到小排序：<br>$$<br>\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_d<br>$$</li>\n</ul>\n</li>\n<li><p>构建映射矩阵：</p>\n<ul>\n<li>取前$l$个最大特征根所对应特征向量$\\mathbf{w}_1, \\mathbf{w}_2, …, \\mathbf{w}_l$组成映射矩阵$\\mathbf{W}$。</li>\n</ul>\n</li>\n<li><p>数据降维：</p>\n<ul>\n<li>将每个人脸图像$x_i$按照如下方法降维：<br>$$<br>(\\mathbf{x}<em>i)</em>{1 \\times d} (\\mathbf{W})_{d \\times l} &#x3D; 1 \\times l<br>$$<br>（其实用的是pca，多的一步就是输入的时候把32*32的图摊开成1024*1的列向量而已）</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"潜在语义分析（Latent-Semantic-Analysis-LSA）\"><a href=\"#潜在语义分析（Latent-Semantic-Analysis-LSA）\" class=\"headerlink\" title=\"潜在语义分析（Latent Semantic Analysis, LSA）\"></a>潜在语义分析（Latent Semantic Analysis, LSA）</h1><p>步骤</p>\n<ol>\n<li><p>构建单词-文档矩阵：</p>\n<ul>\n<li>构建一个单词-文档矩阵$A$，其中每个元素$a_{ij}$表示第$i$个单词在第$j$个文档中的频率（通常使用词频-逆文档频率TF-IDF进行加权）。</li>\n</ul>\n</li>\n<li><p>奇异值分解（SVD）：</p>\n<ul>\n<li>对单词-文档矩阵$A$进行奇异值分解，即$A &#x3D; U \\Sigma V^T$，其中$U$和$V$分别是左奇异向量和右奇异向量组成的矩阵，$\\Sigma$是对角矩阵，其对角线上的元素是$A$的奇异值（按降序排列）。</li>\n</ul>\n</li>\n<li><p>选择前$k$个最大奇异值及对应的奇异向量：</p>\n<ul>\n<li>选取前$k$个最大的奇异值及其对应的奇异向量，形成低秩逼近矩阵$A_k &#x3D; U_k \\Sigma_k V_k^T$。这里$k$的选择取决于保留多少原始信息量，通常根据累积能量准则或经验确定。</li>\n</ul>\n</li>\n<li><p>重建矩阵并挖掘语义关系：</p>\n<ul>\n<li>使用$A_k$代替原始矩阵$A$，可以计算任意两个文档之间的相似度（如皮尔逊相关系数），从而发现文档-文档之间的关联关系。</li>\n<li>同样地，也可以用于探索单词-单词、单词-文档间的隐含关系。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"期望最大化算法（Expectation-Maximization-Algorithm-EM）\"><a href=\"#期望最大化算法（Expectation-Maximization-Algorithm-EM）\" class=\"headerlink\" title=\"期望最大化算法（Expectation-Maximization Algorithm, EM）\"></a>期望最大化算法（Expectation-Maximization Algorithm, EM）</h1><p>EM算法是一种迭代方法，主要用于含有隐变量的概率模型参数估计问题。它分为&#x3D;&#x3D;E步（求期望）&#x3D;&#x3D;和&#x3D;&#x3D;M步（最大化）&#x3D;&#x3D;，通过迭代方式逼近模型参数的最大似然估计值。</p>\n<p>步骤</p>\n<ol>\n<li><p>初始化模型参数：</p>\n<ul>\n<li>首先为模型参数设定初始值（例如高斯混合模型中的均值、方差等）。</li>\n</ul>\n</li>\n<li><p>E步（Expectation Step）：计算隐变量</p>\n<ul>\n<li>基于当前的模型参数，计算隐变量的后验概率分布。对于每一个样本$x_i$和可能的隐变量$z_i$，计算$p(z_i|x_i, \\theta)$，其中$\\theta$表示当前的模型参数。</li>\n</ul>\n</li>\n<li><p>M步（Maximization Step）：最大化似然函数和更新模型参数</p>\n<ul>\n<li>根据观测数据$x_i$、隐变量$z_i$的后验概率分布，重新估计模型参数$\\theta$，以最大化完整数据的对数似然函数$\\log p(x,z|\\theta)$的期望。</li>\n</ul>\n</li>\n<li><p>重复E步和M步：</p>\n<ul>\n<li>不断重复执行E步和M步，直到模型参数收敛或者达到预定的迭代次数为止。</li>\n</ul>\n</li>\n</ol>\n<p>具体的没看懂，等我懂了再说</p>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/06/12/AI/week6-7/",
            "url": "http://example.com/2025/06/12/AI/week6-7/",
            "title": "Week6-7",
            "date_published": "2025-06-11T16:00:00.000Z",
            "content_html": "<h1 id=\"Ch4-机器学习\"><a href=\"#Ch4-机器学习\" class=\"headerlink\" title=\"Ch4 机器学习\"></a>Ch4 机器学习</h1><h2 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h2><ul>\n<li>标注数据</li>\n<li>学习模型</li>\n<li>损失函数<br>典型的损失函数<br><img data-src=\"/figure2.png\"></li>\n</ul>\n<p>经验风险(empirical risk )</p>\n<ul>\n<li>训练集中数据产生的损失。</li>\n<li>经验风险越小说明学习模型对训练数据拟合程度越好。</li>\n</ul>\n<p>期望风险(expected risk):</p>\n<ul>\n<li>当测试集中存在无穷多数据时产生的损失。</li>\n<li>期望风险越小，学习所得模型越好。</li>\n</ul>\n<p>经验风险最小化</p>\n<p>$$\\min_{f \\in \\Phi} \\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i))$$</p>\n<p>期望风险最小化</p>\n<p>$$\\min_{f \\in \\Phi} \\int_{x \\times y} Loss(y, f(x)) P(x, y) dx dy$$</p>\n<p>模型泛化能力与经验风险、期望风险的关系</p>\n<table>\n<thead>\n<tr>\n<th>经验风险小（训练集上表现好）</th>\n<th>期望风险小（测试集上表现好）</th>\n<th>泛化能力强</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>经验风险小（训练集上表现好）</td>\n<td>期望风险大（测试集上表现不好）</td>\n<td>过学习（模型过于复杂）</td>\n</tr>\n<tr>\n<td>经验风险大（训练集上表现不好）</td>\n<td>期望风险大（测试集上表现不好）</td>\n<td>欠学习</td>\n</tr>\n<tr>\n<td>经验风险大（训练集上表现不好）</td>\n<td>期望风险小（测试集上表现好）</td>\n<td>“神仙算法”或“黄粱美梦”</td>\n</tr>\n</tbody></table>\n<p>结构风险最小化 (structural risk minimization)</p>\n<p>为了防止过拟合，在经验风险上加上表示模型复杂度的正则化项 (regularizer) 或惩罚项 (penalty term):</p>\n<p>$$\\min_{f \\in \\Phi} \\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i)) + \\lambda J(f)$$</p>\n<ul>\n<li>经验风险: $\\frac{1}{n} \\sum_{i&#x3D;1}^{n} Loss(y_i, f(x_i))$</li>\n<li>模型复杂度: $\\lambda J(f)$</li>\n</ul>\n<p>监督学习方法又可以分为 生成方法 (generative approach) 和 判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative model)和判别模型(discriminative model)<br><img data-src=\"/figure1.png\"></p>\n<h1 id=\"回归分析\"><a href=\"#回归分析\" class=\"headerlink\" title=\"回归分析\"></a>回归分析</h1><h2 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h2><ul>\n<li>一元线性回归</li>\n</ul>\n<p>$$y_i &#x3D; ax_i + b \\quad (1 \\leq i \\leq n)$$</p>\n<p>$$a &#x3D; \\frac{\\sum_{i&#x3D;1}^{n} x_i y_i - n \\bar{x} \\bar{y}}{\\sum_{i&#x3D;1}^{n} x_i^2 - n \\bar{x}^2}$$</p>\n<p>$$b &#x3D; \\bar{y} - a \\bar{x}$$</p>\n<ul>\n<li><p>多元线性回归<br>$$f(x_i) &#x3D; a_0 + \\sum_{j&#x3D;1}^{D} a_j x_{i,j} &#x3D; a_0 + \\mathbf{a}^T \\mathbf{x}_i$$<br>a是要求的参数，x是输入的数据，f是预测值。<br>为了方便，使用矩阵来表示所有的训练数据和数据标签。<br>$$X &#x3D; [x_1, …, x_m], \\quad y &#x3D; [y_1, …, y_m]$$<br>最小化均方误差得到：<br>$$a &#x3D; (XX^T)^{-1}X^Ty$$</p>\n</li>\n<li><p>逻辑斯蒂回归&#x2F;对数几率回归<br>线性回归一个明显的问题是对离群点导致模型建模不稳定，使结果有偏，为了缓解这个问题（特别是在二分类场景中）带来的影响，可考虑逻辑斯蒂回归<br>逻辑斯蒂回归就是在回归模型中引入 sigmoid函数的一种非线性回归模型</p>\n</li>\n</ul>\n<h2 id=\"逻辑斯蒂回归-Logistic-Regression\"><a href=\"#逻辑斯蒂回归-Logistic-Regression\" class=\"headerlink\" title=\"逻辑斯蒂回归 (Logistic Regression)\"></a>逻辑斯蒂回归 (Logistic Regression)</h2><p>逻辑斯蒂回归（logistic regression）就是在回归模型中引入 sigmoid 函数的一种非线性回归模型。Logistic 回归模型可如下表示：</p>\n<p>$$ y &#x3D; \\frac{1}{1 + e^{-z}} &#x3D; \\frac{1}{1 + e^{-(w^T x + b)}} $$<br>其中 $y \\in (0, 1)$，$z &#x3D; w^T x + b$。<br>这里 $\\frac{1}{1 + e^{-z}}$ 是 sigmoid 函数，$x \\in \\mathbb{R}^d$ 是输入数据，$w \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$ 是回归函数的参数。</p>\n<p>逻辑斯蒂回归多用于&#x3D;&#x3D;二分类&#x3D;&#x3D;问题<br>Sigmoid 函数将任意实数映射到区间(0,1)，这正好符合“概率”的取值范围，所以函数的输出y可以被解释为输入数据x属于正例的概率<br>因此我们可以将输出 y 解释为：</p>\n<blockquote>\n<p>在给定输入特征 x 的条件下，该样本属于正类（例如类别 1）的概率。<br>即：<br>$$<br>y &#x3D; P(y &#x3D; 1 \\mid x)<br>$$<br>如果 $P(y&#x3D;1|x)$ 表示给定输入 $x$ 属于正类的概率，则 $1 - P(y&#x3D;1|x)$ 表示属于负类的概率。<br>$\\frac{P(y&#x3D;1|x)}{1 - P(y&#x3D;1|x)}$ 就是正类相对于负类的优势比。所以&gt;1就归为正类，反之就是负类。</p>\n</blockquote>\n<p>$$<br>\\log \\frac{P(y&#x3D;1|x)}{P(y&#x3D;0|x)} &#x3D; {w^T x + b} &gt; \\log{1} &#x3D; 0<br>$$<br>从这里可以看出，logistic回归本质上是一个线性模型。在预测时，可以计算线性函数$w^T x + b$取值是否大于0来判断输入数据x的类别归属</p>\n<p>为了找到最优参数w和b，我们使用最大似然估计，假设每个样本独立同分布，则<br>……<br>公式懒得敲了，</p>\n<p>为什么基于相关性的方法可能会导致模型的不可解释性和不稳定性</p>\n<ul>\n<li>因果特征和非因果特征</li>\n<li>Making V⊥Y: 最终目标是让非因果特征 V 与输出 Y 独立，即消除虚假相关性，使得模型更加稳定和可解释</li>\n</ul>\n<h1 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h1><p>决策树是一种通过树形结构来进行分类的方法</p>\n<ul>\n<li>信息熵（entropy）是度量样本集合纯度最常用的一种指标<br>假设有一个K个信息（类别），其组成了集合样本D，记第k个信息（类别）发生的概率为$p_k (1 \\leq k \\leq K)$。如下定义这K个信息的信息熵：</li>\n</ul>\n<p>$$Ent(D) &#x3D; -\\sum_{k&#x3D;1}^{K} p_k \\log_2 p_k$$</p>\n<p>&#x3D;&#x3D;$Ent(D)$值越小，表示D包含的信息越确定，也称D的纯度越高。&#x3D;&#x3D;所有$p_k$累加起来的和为1。</p>\n<ul>\n<li>信息增益:衡量使用某个属性进行划分后，数据集不确定性减少的程度<br>得到信息熵后可以进一步计算信息增益：<br>$$Gain(D, A) &#x3D; Ent(D) - \\sum_{i&#x3D;1}^{n} \\frac{|D_i|}{|D|} Ent(D_i)$$<br><img data-src=\"/f3.png\"><br><img data-src=\"/f4.png\"><br>ID3决策树学习算法[Quinlan, 1986]以信息增益为准则来选择划分属性<br>目标：通过不断划分，使得每个子集尽可能“纯净”，即子集内的样本属于同一类</li>\n</ul>\n<p>信息熵（和上面的一样的）<br>$$<br>info &#x3D; -\\sum_{i&#x3D;1}^{n} \\frac{|D_i|}{|D|} \\log_2 \\frac{|D_i|}{|D|}<br>$$</p>\n<p>增益率（Gain-ratio）：</p>\n<p>$$<br>Gain-ratio &#x3D; \\frac{Gain(D, A)}{info}<br>$$<br>存在的问题：增益率准则对可取数目较少的属性有所偏好</p>\n<p>另一种计算更简的度量指标是如下的 Gini 指数（基尼指数）：</p>\n<p>$$<br>Gini(D) &#x3D; 1 - \\sum_{k&#x3D;1}^{K} p_k^2<br>$$</p>\n<p>相对于信息熵的计算 $E(D) &#x3D; -\\sum_{k&#x3D;1}^{K} p_k \\log_2 p_k$，不用计算对数 log，计算更为简易。</p>\n<h2 id=\"连续属性离散化\"><a href=\"#连续属性离散化\" class=\"headerlink\" title=\"连续属性离散化\"></a>连续属性离散化</h2><ol>\n<li><p>确定连续属性的取值范围，确定划分点集合<br>考虑包含 n-1 个元素的候选划分点集合：<br>$$<br>T_a &#x3D; \\left{ \\frac{a^i + a^{i+1}}{2} ,\\middle|, 1 \\leq i \\leq n - 1 \\right}<br>$$<br>这里的每个候选划分点是相邻两个取值的中点，即区间 $[a^i, a^{i+1})$ 的中位点 $\\frac{a^i + a^{i+1}}{2}$</p>\n</li>\n<li><p>计算信息增益<br>$$<br>\\text{Gain}(D, a, t) &#x3D; \\text{Ent}(D) - \\sum_{\\lambda \\in {-, +}} \\frac{|D_t^\\lambda|}{|D|} \\cdot \\text{Ent}(D_t^\\lambda)<br>$$<br>计算每个划分点的信息增益率，选择信息增益最大的划分点</p>\n</li>\n</ol>\n<p>+++info example<br>;;;id3 example<br>给定数据点及其对应的类别标签如下：</p>\n<ul>\n<li>$a_1 &#x3D; 1$ -&gt; 类别为 0</li>\n<li>$a_2 &#x3D; 3$ -&gt; 类别为 1</li>\n<li>$a_3 &#x3D; 5$ -&gt; 类别为 0</li>\n<li>$a_4 &#x3D; 7$ -&gt; 类别为 1</li>\n<li>$a_5 &#x3D; 9$ -&gt; 类别为 0</li>\n</ul>\n<p>因此，我们的数据集 $D$ 是 ${1, 3, 5, 7, 9}$，对应的类别标签分别为 ${0, 1, 0, 1, 0}$。</p>\n<p>第一步：计算原始数据集的信息熵</p>\n<p>$$<br>Ent(D) &#x3D; -\\left( p_0 \\log_2 p_0 + p_1 \\log_2 p_1 \\right)<br>$$</p>\n<p>其中，$p_0 &#x3D; \\frac{3}{5}$，$p_1 &#x3D; \\frac{2}{5}$，则：</p>\n<p>$$<br>Ent(D) &#x3D; -\\left( \\frac{3}{5} \\log_2 \\frac{3}{5} + \\frac{2}{5} \\log_2 \\frac{2}{5} \\right) \\approx 0.971<br>$$</p>\n<p>第二步：确定候选划分点集合</p>\n<p>根据公式 $T_a &#x3D; \\left{ \\frac{a^i + a^{i+1}}{2} ,\\middle|, 1 \\leq i \\leq n - 1 \\right}$，我们得到候选划分点集合：</p>\n<p>$$<br>T_a &#x3D; {2, 4, 6, 8}<br>$$</p>\n<p>第三步：计算每个候选划分点的信息增益</p>\n<p>以划分点 $t &#x3D; 4$ 为例：</p>\n<ul>\n<li>$D_t^{-} &#x3D; {1, 3}$，类别为 ${0, 1}$</li>\n<li>$D_t^{+} &#x3D; {5, 7, 9}$，类别为 ${0, 1, 0}$</li>\n</ul>\n<p>计算这两个子集的熵：</p>\n<ul>\n<li>$$Ent(D_t^{-}) &#x3D; -\\left( \\frac{1}{2} \\log_2 \\frac{1}{2} + \\frac{1}{2} \\log_2 \\frac{1}{2} \\right) &#x3D; 1$$</li>\n<li>$$Ent(D_t^{+}) &#x3D; -\\left( \\frac{2}{3} \\log_2 \\frac{2}{3} + \\frac{1}{3} \\log_2 \\frac{1}{3} \\right) \\approx 0.918$$</li>\n</ul>\n<p>计算信息增益：</p>\n<p>$$<br>Gain(D, a, t&#x3D;4) &#x3D; Ent(D) - \\left( \\frac{|D_t^{-}|}{|D|} \\cdot Ent(D_t^{-}) + \\frac{|D_t^{+}|}{|D|} \\cdot Ent(D_t^{+}) \\right)<br>$$</p>\n<p>代入数值：</p>\n<p>$$<br>Gain(D, a, t&#x3D;4) &#x3D; 0.971 - \\left( \\frac{2}{5} \\cdot 1 + \\frac{3}{5} \\cdot 0.918 \\right) \\approx 0.029<br>$$</p>\n<p>重复上述过程，对所有划分点 $t &#x3D; 2, 4, 6, 8$ 进行类似计算，并选择使 $Gain(D, a, t)$ 最大的那个作为最优划分点。<br>;;;<br>+++</p>\n<h1 id=\"线性区别分析-LDA-FDA\"><a href=\"#线性区别分析-LDA-FDA\" class=\"headerlink\" title=\"线性区别分析 (LDA&#x2F;FDA)\"></a>线性区别分析 (LDA&#x2F;FDA)</h1><p>线性判别分析(linear discriminant analysis， LDA)是一种基于监督学习的降维方法，也称为Fisher线性判别分析(fisher’s discriminant analysis，FDA),对于一组具有标签信息的高维数据样本，LDA利用其类别信息，将其线性投影到一个低维空间上，在低维空间中同一类别样本尽可能靠近，不同类别样本尽可能彼此远离。</p>\n<ol>\n<li>计算数据样本集中每个类别样本的均值</li>\n<li>计算类内散度矩阵$S_w$和类间散度矩阵$S_b$</li>\n<li>根据$S_w^{-1}S_bW&#x3D;\\lambda W$来求解$S_w^{-1}S_b$所对应前$r$个最大特征值所对应特征向量$(w_1,w_2,…,w_r)$，构成矩阵W</li>\n<li>通过矩阵$W$将每个样本映射到低维空间，实现特征降维。</li>\n</ol>\n<p>具体不想看，考到就给了</p>\n<h1 id=\"Ada-Boosting\"><a href=\"#Ada-Boosting\" class=\"headerlink\" title=\"Ada Boosting\"></a>Ada Boosting</h1><p>。。看不懂懒得看</p>\n<h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a>支持向量机</h1><h1 id=\"生成学习模型\"><a href=\"#生成学习模型\" class=\"headerlink\" title=\"生成学习模型\"></a>生成学习模型</h1>",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/06/11/AI/week4-5/",
            "url": "http://example.com/2025/06/11/AI/week4-5/",
            "title": "Week4-5",
            "date_published": "2025-06-10T16:00:00.000Z",
            "content_html": "<p>#Ch3 搜索算法</p>\n<h2 id=\"无信息搜索\"><a href=\"#无信息搜索\" class=\"headerlink\" title=\"无信息搜索\"></a>无信息搜索</h2><p>BFS DFS 略</p>\n<h2 id=\"启发式搜索\"><a href=\"#启发式搜索\" class=\"headerlink\" title=\"启发式搜索\"></a>启发式搜索</h2><ul>\n<li>贪婪优先搜索<ul>\n<li>每次取最短的；缺点：不一定是最优的</li>\n<li>时间和空间复杂度均为 $O(b_m)$，b是搜索树分支因子，m是最大深度<br><img data-src=\"/figure1.png\"><br>:::info<br>每次取当前节点的下一个节点到终点中直线距离最短的<br>:::</li>\n</ul>\n</li>\n<li>A*算法<ul>\n<li>评价函数：f(n) &#x3D; g(n) + h(n)</li>\n<li>代价函数 g(n) 表示从起始结点到结点n的开销代价值</li>\n<li>启发函数 h(n) 表示从结点n到目标结点路径中所估算的最小开销代价值。</li>\n<li>评价函数 f(n) 可视为经过结点n、具有最小开销代价值的路径。<ul>\n<li>在最短路径问题中，g(?)为当前选择的路径的实际距离，即从上一个节点到下一个节点的实际距离，?(?)为下一个节点到目标城市的直线距离。每一次搜索，下一个节点选择与此刻城市连接的所有节点中，g(?)+?(?)最小的城市节点。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>:::info<br>取（当前节点到下一节点的距离+下一节点到目标城市的距离）最短的<br>:::<br>A*算法的完备性和最优性取决于搜索问题和启发函数的性质<br>一个良好的启发函数需要满足:可容性（admissible）;一致性（consistency）<br>如果启发函数是可容的，那么树搜索的A*算法满足最优性(最优性:搜索算法是否能保证找到的第一个解是最优解)<br>满足一致性条件的启发函数一定满足可容性条件，反之不一定</p>\n<h2 id=\"对抗搜索\"><a href=\"#对抗搜索\" class=\"headerlink\" title=\"对抗搜索\"></a>对抗搜索</h2><ul>\n<li><p>最小最大搜索（minimax）</p>\n<ul>\n<li>最小最大搜索是一个在你和对手轮流行动的情况下，为你自己寻找最优策略的算法。</li>\n<li>算法：略</li>\n<li>时间复杂度：$O(b^m)$</li>\n<li>空间复杂度：$O(bm)$</li>\n</ul>\n</li>\n<li><p>\\alpha-\\beta剪枝</p>\n<ul>\n<li>Minimax 会穷举整个博弈树，但我们可以用剪枝技巧跳过一些无用分支，让它跑得更快</li>\n<li>max层的下界取下一层（上界）里面最大的；min层的上界取下一层（下界）里面最小的<br>懒得写直接看例子：<br><img data-src=\"/figure2.png\"><br> Alpha-Beta 剪枝算法什么时候扩展的结点数量最少？</li>\n<li>每一层最左端结点的所有孩子结点均被访问，其他节点仅有最左端孩子结点被访问、其他孩子结点被剪枝。<br> 如果一个节点导致了其兄弟节点被剪枝，可知其孩子节点必然被扩展。</li>\n<li>最优效率下时间复杂度：$O(b^{m&#x2F;2})$  (或者m+1);最差的就是完全没剪枝和minimax一样</li>\n</ul>\n</li>\n<li><p>蒙特卡洛树搜索</p>\n<ul>\n<li>选择(UCB)、扩展、模拟(随机)、反向传播</li>\n<li>悔值函数<br>:::info<br>没完全懂，后面再回来研究<br>:::</li>\n</ul>\n</li>\n</ul>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/05/20/AI/week1/",
            "url": "http://example.com/2025/05/20/AI/week1/",
            "title": "Week1",
            "date_published": "2025-05-19T16:00:00.000Z",
            "content_html": "<blockquote>\n<p>2025-2026春夏人工智能课程笔记</p>\n</blockquote>\n<h1 id=\"Ch1-绪论\"><a href=\"#Ch1-绪论\" class=\"headerlink\" title=\"Ch1 绪论\"></a>Ch1 绪论</h1><ul>\n<li>人工智能求解：<ul>\n<li>以符号主义为核心的逻辑推理：将概念（如命题等）符号化，从若干判断（前提）出发得到新判断（结论）</li>\n<li>以问题求解为核心的探寻搜索:探寻搜索依据已有信息来寻找满足约束条件的待求解问题的答案</li>\n<li>以数据驱动为核心的机器学习:从数据中发现数据所承载语义（如概念）的内在模式</li>\n<li>以行为主义为核心的强化学习:根据环境所提供的奖罚反馈来学习所处状态可施加的最佳行动，在“探索（未知空间）-利用（已有经验）（exploration vs. exploitation）”之间寻找平衡，完成某个序列化任务，具备自我学习能力</li>\n<li>以博弈对抗为核心的群体智能:从“数据拟合”优化解的求取向“均衡解”的求取迈进</li>\n</ul>\n</li>\n</ul>\n",
            "tags": [
                "人工智能"
            ]
        },
        {
            "id": "http://example.com/2025/05/20/AI/week2-3/",
            "url": "http://example.com/2025/05/20/AI/week2-3/",
            "title": "Week2-3",
            "date_published": "2025-05-19T16:00:00.000Z",
            "content_html": "<h1 id=\"Ch2-知识表达与推理\"><a href=\"#Ch2-知识表达与推理\" class=\"headerlink\" title=\"Ch2 知识表达与推理\"></a>Ch2 知识表达与推理</h1><h2 id=\"命题逻辑\"><a href=\"#命题逻辑\" class=\"headerlink\" title=\"命题逻辑\"></a>命题逻辑</h2><p><img data-src=\"/img1.png\"><br>真值表：<br><img data-src=\"/img2.png\"></p>\n<blockquote>\n<p>“条件”命题联结词中前提为假时命题结论永远为真，bi-conditional只有两个都是true或者都是false才是true<br>逻辑等价：给定命题p和命题q，如果&#x3D;&#x3D;p和q在所有情况下都具有同样真假结果&#x3D;&#x3D;，那么p和q在逻辑上等价，一般用 $\\equiv$ 来表示，即p $\\equiv$ q。<br>判断逻辑等价：画真值表<br>逻辑等价式：<br><img data-src=\"/img3.jpg\"><br><img data-src=\"/img4.png\"></p>\n</blockquote>\n<ul>\n<li>normal form<ul>\n<li>有限个简单合取式构成的析取式称为析取(or)范式</li>\n<li>由有限个简单析取式构成的合取式称为合取(and)范式</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"谓词逻辑\"><a href=\"#谓词逻辑\" class=\"headerlink\" title=\"谓词逻辑\"></a>谓词逻辑</h2><ul>\n<li>全称量词与存在量词</li>\n<li>约束变元、自由变元<br>:::info<br>在约束变元相同的情况下，量词的运算满足分配律：全称量词对析取没有分配律、存在量词对合取没有分配律<br>:::<br>$$\\begin{aligned}<br>(\\forall x)(A(x) \\lor B(x)) \\equiv (\\forall x)A(x) \\lor (\\forall x)B(x) 不成立<br>\\end{aligned}$$</li>\n</ul>\n<p>$$\\begin{aligned}<br>(\\forall x)(A(x) \\land B(x)) \\equiv (\\forall x)A(x) \\land (\\forall x)B(x) 成立<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(A(x) \\lor B(x)) \\equiv (\\exists x)A(x) \\lor (\\exists x)B(x) 成立<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(A(x) \\land B(x)) \\equiv (\\exists x)A(x) \\land (\\exists x)B(x) 不成立<br>\\end{aligned}$$<br>:::info<br>当公式中存在多个量词时，若多个量词都是全称量词或者都是存在量词，则量词的位置可以互换；若多个量词中既有全称量词又有存在量词，则量词的位置不可以随意互换<br>:::<br>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\forall y)(\\forall x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(\\exists y)A(x, y) \\equiv (\\exists y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\exists y)(\\forall x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\forall y)A(x, y) \\equiv (\\exists x)(\\forall y)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists y)(\\forall x)A(x, y) \\equiv (\\forall x)(\\exists y)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\exists x)(\\forall y)A(x, y) \\equiv (\\forall y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall x)(\\exists y)A(x, y) \\equiv (\\exists y)(\\exists x)A(x, y)<br>\\end{aligned}$$</p>\n<p>$$\\begin{aligned}<br>(\\forall y)(\\exists x)A(x, y) \\equiv (\\exists x)(\\exists y)A(x, y)<br>\\end{aligned}$$</p>\n<ul>\n<li>利用谓词逻辑进行推理<ul>\n<li>全称量词消去： $(\\forall x) A(x) \\equiv A(y)$</li>\n<li>全称量词引入： $A(y) \\equiv (\\forall x) A(x)$</li>\n<li>存在量词消去： $(\\exists x) A(x) \\equiv A(c)$</li>\n<li>存在量词引入： $A(c) \\equiv (\\exists x) A(x)$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"知识图谱推理\"><a href=\"#知识图谱推理\" class=\"headerlink\" title=\"知识图谱推理\"></a>知识图谱推理</h2><ul>\n<li>知识图谱可视为包含多种关系的图。在图中，每个节点是一个实体（如人名、地名、事件和活动等），任意两个节点之间的边表示这两个节点之间存在的关系。</li>\n<li>可将知识图谱中任意两个相连节点及其连接边表示成一个三元组（triplet）,即 (left_node, relation, right_node)<br>两类代表性方法：</li>\n<li>归纳逻辑程序设计 (inductive logic programming，ILP)算法</li>\n<li>路径排序算法（path ranking algorithm, PRA）</li>\n</ul>\n<p>ILP: 一阶归纳学习FOIL（First Order Inductive Learner）<br>推理手段: 正例集合 + 反例集合 + 背景知识样例 ⟹ 目标谓词作为结论的推理规则<br><img data-src=\"/img5.png\"><br>懒得写了，看ppt吧<br><img data-src=\"/img6.png\"><br>推理规则覆盖所有正例且不覆盖任何反例的时候算法结束</p>\n<p>PRA: 路径排序算法<br><img data-src=\"/img7.png\"><br>(4)的意思是看两个实体能不能通过(3)的关系从第一个走到第二个。<br>后面的1表示正例，-1表示负例。</p>\n<h2 id=\"概率图推理\"><a href=\"#概率图推理\" class=\"headerlink\" title=\"概率图推理\"></a>概率图推理</h2><p>贝叶斯网络<br><img data-src=\"/img14.png\"><br>要会算</p>\n<p>马尔科夫逻辑网络</p>\n<h2 id=\"因果推理\"><a href=\"#因果推理\" class=\"headerlink\" title=\"因果推理\"></a>因果推理</h2><p>因果定义：变量X是变量Y的原因，当且仅当保持其它所有变量不变的情况下，改变X的值能导致Y的值发生变化。<br>因果效应：因变量X改变一个单位时，果变量Y的变化程度</p>\n<p>因果图是有向无环图</p>\n<p>结构因果模型：结构因果模型由两组变量集合U和V以及一组函数f组成。其中，f是根据模型中其他变量取值而给V中每一个变量赋值的函数<br>结构因果模型中的原因：如果变量X出现在给变量X赋值的函数中，如$Y &#x3D; f(X) + \\epsilon$，则X是Y的直接原因<br>因果图中的联合概率分布：直接看图<br><img data-src=\"/img8.png\"><br>因果图的基本结构：</p>\n<ul>\n<li>链结构<br>  - <img data-src=\"/img9.png\"><br>  - 对于变量X和Y，若X和Y之间只有一条单向的路径，变量Z是截断(intercept)该路径的集合中的任一变量，则在给定Z时，X和Y条件独立。</li>\n</ul>\n<p>$$<br>P(X, Y | Z) &#x3D; P(X | Z)P(Y | Z)<br>$$</p>\n<ul>\n<li>分连结构<br>  - <img data-src=\"/img10.png\"></li>\n</ul>\n<p>$$<br>P(X, Y | Z) &#x3D; \\frac {P(X, Y, Z)}{P(Z)} &#x3D; \\frac {P(X | Z)P(Y | Z)P(Z)}{P(Z)} &#x3D; P(X | Z)P(Y | Z)<br>$$</p>\n<ul>\n<li>汇联结构<br>  - <img data-src=\"/img11.jpg\"></li>\n</ul>\n<p>$$<br>P(X, Y | Z) &#x3D; \\frac{P(X, Y, Z)} {P(Z)} &#x3D; \\frac {P(X, Y, Z)}{P(Z)} &#x3D; \\frac {P(X)P(Y)P(Z&#x2F;X, Y)}{P(Z)} \\neq P(X | Z)P(Y | Z)<br>$$</p>\n<h3 id=\"D-分离-directional-separation-d-separation-，可用于判断任意两个节点的相关性和独立性\"><a href=\"#D-分离-directional-separation-d-separation-，可用于判断任意两个节点的相关性和独立性\" class=\"headerlink\" title=\"D-分离(directional separation, d-separation)，可用于判断任意两个节点的相关性和独立性\"></a>D-分离(directional separation, d-separation)，可用于判断任意两个节点的相关性和独立性</h3><ul>\n<li>限定集：已知或观察到的变量集合（给定的变量集合）</li>\n<li>路径p被限定集Z阻塞(block)当且仅当：<ul>\n<li>(1) 路径p含有链结构A → B → C或分连结构A ← B → C且中间节点B在Z中，或</li>\n<li>(2) 路径p含有汇连结构A → B ← C且汇连节点B及其后代都不在Z中。</li>\n<li>若Z阻塞了节点X和节点Y之间的每一条路径，则称给定Z时，X和Y是D-分离，即给定Z时，X和Y条件独立</li>\n<li>&#x3D;&#x3D;链式、分连中间节点在，汇联中间节点和后代不在则D-分离&#x3D;&#x3D;</li>\n</ul>\n</li>\n</ul>\n<p>因果定义：变量X是变量Y的原因，当且仅当保持其它所有变量不变的情况下，改变X的值能导致Y的值发生变化。<br>因果效应：因变量X改变一个单位时，果变量Y的变化程度因果推理的两个关键因素：</p>\n<ul>\n<li>改变因变量T</li>\n<li>保证其它变量不变<br>干预：干预(intervention)指的是固定(fix)系统中的变量，然后改变系统，观察其他变量的变化。<br>为了与X自然取值x时进行区分，在对X进行干预时，引入“do算子”(do-calculus)，记作do(X &#x3D; x)。<br>因此，P(Y &#x3D; y|X &#x3D; x)表示的是当发现X &#x3D; x时，Y&#x3D; y的概率；而P(Y &#x3D; y|do(X &#x3D;x))表示的是对X进行干预，固定其值为x时，Y &#x3D; y的概率。<br>用统计学的术语来说，P(Y &#x3D; y|X &#x3D; x)反映的是在取值为x的个体X上，Y的总体分布；而P(Y &#x3D; y|do(X &#x3D;x))反映的是如果将每一个X取值都固定为x时，Y的总体分布。</li>\n</ul>\n<p>因果效应差&#x2F;平均因果效应 (ACE)  懒得写了看图吧<br><img data-src=\"/img12.png\"><br><img data-src=\"/img13.png\"><br>计算因果效应的关键在于计算操纵概率(manipulatedprobability) $P_m$<br>调整公式：<br>$$<br>P(Y &#x3D; y \\mid do(X &#x3D; x)) &#x3D; \\sum_z P(Y &#x3D; y \\mid X &#x3D; x, Z &#x3D; z) \\cdot P(Z &#x3D; z)<br>$$<br>对于Z的每一个取值z，计算X和Y的条件概率并取均值<br>+++info example<br>;;;id3 例题<br>假设我们研究以下变量：</p>\n<ul>\n<li>X：是否服药  <ul>\n<li>$X &#x3D; 1$：服药  </li>\n<li>$X &#x3D; 0$：不服药</li>\n</ul>\n</li>\n<li>Y：是否康复  <ul>\n<li>$Y &#x3D; 1$：康复  </li>\n<li>$Y &#x3D; 0$：未康复</li>\n</ul>\n</li>\n<li>Z：性别  <ul>\n<li>$Z &#x3D; 0$：男  </li>\n<li>$Z &#x3D; 1$：女<br>我们知道性别会影响：</li>\n</ul>\n</li>\n<li>是否选择服药（比如男性更倾向于尝试新药）</li>\n<li>康复率（比如女性可能有更强的免疫力）<br>因此，性别 Z 是一个混杂变量，需要在分析中进行控制。<br>已知：<table>\n<thead>\n<tr>\n<th>Z（性别）</th>\n<th>P(Z)</th>\n<th>P(Y&#x3D;1 | X&#x3D;1, Z)</th>\n<th>P(Y&#x3D;1 | X&#x3D;0, Z)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>男（0）</td>\n<td>0.6</td>\n<td>0.7</td>\n<td>0.4</td>\n</tr>\n<tr>\n<td>女（1）</td>\n<td>0.4</td>\n<td>0.5</td>\n<td>0.3</td>\n</tr>\n<tr>\n<td>我们想知道：</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>如果强制所有人都服药（即 $do(X&#x3D;1)$），整体康复率是多少？</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>也就是要计算：</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$$</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>P(Y&#x3D;1 \\mid do(X&#x3D;1))</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>$$</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>;;;</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>;;;id3 答案</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>根据调整公式：</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<p>$$<br>P(Y&#x3D;1 \\mid do(X&#x3D;1)) &#x3D; \\sum_z P(Y&#x3D;1 \\mid X&#x3D;1, Z&#x3D;z) \\cdot P(Z&#x3D;z)<br>$$</p>\n<p>代入数据计算</p>\n<p>$$<br>P(Y&#x3D;1 \\mid do(X&#x3D;1)) &#x3D; P(Y&#x3D;1 \\mid X&#x3D;1, Z&#x3D;0) \\cdot P(Z&#x3D;0) + P(Y&#x3D;1 \\mid X&#x3D;1, Z&#x3D;1) \\cdot P(Z&#x3D;1)<br>$$</p>\n<p>$$<br>&#x3D; 0.7 \\times 0.6 + 0.5 \\times 0.4 &#x3D; 0.42 + 0.2 &#x3D; 0.62<br>$$<br>+++</p>\n<p>(因果效应)给定因果图G，PA表示X的父节点集合，则X对Y的因果效应为<br>$$<br>P(Y&#x3D;y \\mid do(X&#x3D;x)) &#x3D; \\sum_z P(Y&#x3D;y \\mid X&#x3D;x, PA&#x3D;z) \\cdot P(PA&#x3D;z)<br>$$<br>后门调整：<br>不写了</p>\n",
            "tags": [
                "人工智能"
            ]
        }
    ]
}