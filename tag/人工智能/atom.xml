<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://example.com</id>
    <title> • Posts by &#34;人工智能&#34; tag</title>
    <link href="http://example.com" />
    <updated>2025-06-11T16:00:00.000Z</updated>
    <category term="人工智能" />
    <category term="�˹�����" />
    <category term="编译原理" />
    <category term="词法分析" />
    <category term="抽象语法" />
    <category term="语义分析" />
    <category term="语法分析" />
    <category term="计算机组成" />
    <category term="数据的表示与运算" />
    <category term="计算机网络" />
    <entry>
        <id>http://example.com/2025/06/12/AI/week6-7/</id>
        <title>Week6-7</title>
        <link rel="alternate" href="http://example.com/2025/06/12/AI/week6-7/"/>
        <content type="html">&lt;h1 id=&#34;Ch4-机器学习&#34;&gt;&lt;a href=&#34;#Ch4-机器学习&#34; class=&#34;headerlink&#34; title=&#34;Ch4 机器学习&#34;&gt;&lt;/a&gt;Ch4 机器学习&lt;/h1&gt;&lt;h2 id=&#34;监督学习&#34;&gt;&lt;a href=&#34;#监督学习&#34; class=&#34;headerlink&#34; title=&#34;监督学习&#34;&gt;&lt;/a&gt;监督学习&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;标注数据&lt;/li&gt;
&lt;li&gt;学习模型&lt;/li&gt;
&lt;li&gt;损失函数&lt;br&gt;典型的损失函数&lt;br&gt;&lt;img data-src=&#34;/figure2.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;经验风险(empirical risk )&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练集中数据产生的损失。&lt;/li&gt;
&lt;li&gt;经验风险越小说明学习模型对训练数据拟合程度越好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;期望风险(expected risk):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当测试集中存在无穷多数据时产生的损失。&lt;/li&gt;
&lt;li&gt;期望风险越小，学习所得模型越好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;经验风险最小化&lt;/p&gt;
&lt;p&gt;$$\min_{f \in \Phi} \frac{1}{n} \sum_{i&amp;#x3D;1}^{n} Loss(y_i, f(x_i))$$&lt;/p&gt;
&lt;p&gt;期望风险最小化&lt;/p&gt;
&lt;p&gt;$$\min_{f \in \Phi} \int_{x \times y} Loss(y, f(x)) P(x, y) dx dy$$&lt;/p&gt;
&lt;p&gt;模型泛化能力与经验风险、期望风险的关系&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;经验风险小（训练集上表现好）&lt;/th&gt;
&lt;th&gt;期望风险小（测试集上表现好）&lt;/th&gt;
&lt;th&gt;泛化能力强&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;经验风险小（训练集上表现好）&lt;/td&gt;
&lt;td&gt;期望风险大（测试集上表现不好）&lt;/td&gt;
&lt;td&gt;过学习（模型过于复杂）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;经验风险大（训练集上表现不好）&lt;/td&gt;
&lt;td&gt;期望风险大（测试集上表现不好）&lt;/td&gt;
&lt;td&gt;欠学习&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;经验风险大（训练集上表现不好）&lt;/td&gt;
&lt;td&gt;期望风险小（测试集上表现好）&lt;/td&gt;
&lt;td&gt;“神仙算法”或“黄粱美梦”&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;结构风险最小化 (structural risk minimization)&lt;/p&gt;
&lt;p&gt;为了防止过拟合，在经验风险上加上表示模型复杂度的正则化项 (regularizer) 或惩罚项 (penalty term):&lt;/p&gt;
&lt;p&gt;$$\min_{f \in \Phi} \frac{1}{n} \sum_{i&amp;#x3D;1}^{n} Loss(y_i, f(x_i)) + \lambda J(f)$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;经验风险: $\frac{1}{n} \sum_{i&amp;#x3D;1}^{n} Loss(y_i, f(x_i))$&lt;/li&gt;
&lt;li&gt;模型复杂度: $\lambda J(f)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;监督学习方法又可以分为 生成方法 (generative approach) 和 判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative model)和判别模型(discriminative model)&lt;br&gt;&lt;img data-src=&#34;/figure1.png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;回归分析&#34;&gt;&lt;a href=&#34;#回归分析&#34; class=&#34;headerlink&#34; title=&#34;回归分析&#34;&gt;&lt;/a&gt;回归分析&lt;/h1&gt;&lt;h2 id=&#34;线性回归&#34;&gt;&lt;a href=&#34;#线性回归&#34; class=&#34;headerlink&#34; title=&#34;线性回归&#34;&gt;&lt;/a&gt;线性回归&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;一元线性回归&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$y_i &amp;#x3D; ax_i + b \quad (1 \leq i \leq n)$$&lt;/p&gt;
&lt;p&gt;$$a &amp;#x3D; \frac{\sum_{i&amp;#x3D;1}^{n} x_i y_i - n \bar{x} \bar{y}}{\sum_{i&amp;#x3D;1}^{n} x_i^2 - n \bar{x}^2}$$&lt;/p&gt;
&lt;p&gt;$$b &amp;#x3D; \bar{y} - a \bar{x}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;多元线性回归&lt;br&gt;$$f(x_i) &amp;#x3D; a_0 + \sum_{j&amp;#x3D;1}^{D} a_j x_{i,j} &amp;#x3D; a_0 + \mathbf{a}^T \mathbf{x}_i$$&lt;br&gt;a是要求的参数，x是输入的数据，f是预测值。&lt;br&gt;为了方便，使用矩阵来表示所有的训练数据和数据标签。&lt;br&gt;$$X &amp;#x3D; [x_1, …, x_m], \quad y &amp;#x3D; [y_1, …, y_m]$$&lt;br&gt;最小化均方误差得到：&lt;br&gt;$$a &amp;#x3D; (XX^T)^{-1}X^Ty$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;逻辑斯蒂回归&amp;#x2F;对数几率回归&lt;br&gt;线性回归一个明显的问题是对离群点导致模型建模不稳定，使结果有偏，为了缓解这个问题（特别是在二分类场景中）带来的影响，可考虑逻辑斯蒂回归&lt;br&gt;逻辑斯蒂回归就是在回归模型中引入 sigmoid函数的一种非线性回归模型&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;逻辑斯蒂回归-Logistic-Regression&#34;&gt;&lt;a href=&#34;#逻辑斯蒂回归-Logistic-Regression&#34; class=&#34;headerlink&#34; title=&#34;逻辑斯蒂回归 (Logistic Regression)&#34;&gt;&lt;/a&gt;逻辑斯蒂回归 (Logistic Regression)&lt;/h2&gt;&lt;p&gt;逻辑斯蒂回归（logistic regression）就是在回归模型中引入 sigmoid 函数的一种非线性回归模型。Logistic 回归模型可如下表示：&lt;/p&gt;
&lt;p&gt;$$ y &amp;#x3D; \frac{1}{1 + e^{-z}} &amp;#x3D; \frac{1}{1 + e^{-(w^T x + b)}} $$&lt;br&gt;其中 $y \in (0, 1)$，$z &amp;#x3D; w^T x + b$。&lt;br&gt;这里 $\frac{1}{1 + e^{-z}}$ 是 sigmoid 函数，$x \in \mathbb{R}^d$ 是输入数据，$w \in \mathbb{R}^d$ 和 $b \in \mathbb{R}$ 是回归函数的参数。&lt;/p&gt;
&lt;p&gt;逻辑斯蒂回归多用于&amp;#x3D;&amp;#x3D;二分类&amp;#x3D;&amp;#x3D;问题&lt;br&gt;Sigmoid 函数将任意实数映射到区间(0,1)，这正好符合“概率”的取值范围，所以函数的输出y可以被解释为输入数据x属于正例的概率&lt;br&gt;因此我们可以将输出 y 解释为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在给定输入特征 x 的条件下，该样本属于正类（例如类别 1）的概率。&lt;br&gt;即：&lt;br&gt;$$&lt;br&gt;y &amp;#x3D; P(y &amp;#x3D; 1 \mid x)&lt;br&gt;$$&lt;br&gt;如果 $P(y&amp;#x3D;1|x)$ 表示给定输入 $x$ 属于正类的概率，则 $1 - P(y&amp;#x3D;1|x)$ 表示属于负类的概率。&lt;br&gt;$\frac{P(y&amp;#x3D;1|x)}{1 - P(y&amp;#x3D;1|x)}$ 就是正类相对于负类的优势比。所以&amp;gt;1就归为正类，反之就是负类。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$&lt;br&gt;\log \frac{P(y&amp;#x3D;1|x)}{P(y&amp;#x3D;0|x)} &amp;#x3D; {w^T x + b} &amp;gt; \log{1} &amp;#x3D; 0&lt;br&gt;$$&lt;br&gt;从这里可以看出，logistic回归本质上是一个线性模型。在预测时，可以计算线性函数$w^T x + b$取值是否大于0来判断输入数据x的类别归属&lt;/p&gt;
&lt;p&gt;为了找到最优参数w和b，我们使用最大似然估计，假设每个样本独立同分布，则&lt;br&gt;……&lt;br&gt;公式懒得敲了，&lt;/p&gt;
&lt;p&gt;为什么基于相关性的方法可能会导致模型的不可解释性和不稳定性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因果特征和非因果特征&lt;/li&gt;
&lt;li&gt;Making V⊥Y: 最终目标是让非因果特征 V 与输出 Y 独立，即消除虚假相关性，使得模型更加稳定和可解释&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;决策树&#34;&gt;&lt;a href=&#34;#决策树&#34; class=&#34;headerlink&#34; title=&#34;决策树&#34;&gt;&lt;/a&gt;决策树&lt;/h1&gt;&lt;p&gt;决策树是一种通过树形结构来进行分类的方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;信息熵（entropy）是度量样本集合纯度最常用的一种指标&lt;br&gt;假设有一个K个信息（类别），其组成了集合样本D，记第k个信息（类别）发生的概率为$p_k (1 \leq k \leq K)$。如下定义这K个信息的信息熵：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$Ent(D) &amp;#x3D; -\sum_{k&amp;#x3D;1}^{K} p_k \log_2 p_k$$&lt;/p&gt;
&lt;p&gt;&amp;#x3D;&amp;#x3D;$Ent(D)$值越小，表示D包含的信息越确定，也称D的纯度越高。&amp;#x3D;&amp;#x3D;所有$p_k$累加起来的和为1。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;信息增益:衡量使用某个属性进行划分后，数据集不确定性减少的程度&lt;br&gt;得到信息熵后可以进一步计算信息增益：&lt;br&gt;$$Gain(D, A) &amp;#x3D; Ent(D) - \sum_{i&amp;#x3D;1}^{n} \frac{|D_i|}{|D|} Ent(D_i)$$&lt;br&gt;&lt;img data-src=&#34;/f3.png&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/f4.png&#34;&gt;&lt;br&gt;ID3决策树学习算法[Quinlan, 1986]以信息增益为准则来选择划分属性&lt;br&gt;目标：通过不断划分，使得每个子集尽可能“纯净”，即子集内的样本属于同一类&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;信息熵（和上面的一样的）&lt;br&gt;$$&lt;br&gt;info &amp;#x3D; -\sum_{i&amp;#x3D;1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;增益率（Gain-ratio）：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;Gain-ratio &amp;#x3D; \frac{Gain(D, A)}{info}&lt;br&gt;$$&lt;br&gt;存在的问题：增益率准则对可取数目较少的属性有所偏好&lt;/p&gt;
&lt;p&gt;另一种计算更简的度量指标是如下的 Gini 指数（基尼指数）：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;Gini(D) &amp;#x3D; 1 - \sum_{k&amp;#x3D;1}^{K} p_k^2&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;相对于信息熵的计算 $E(D) &amp;#x3D; -\sum_{k&amp;#x3D;1}^{K} p_k \log_2 p_k$，不用计算对数 log，计算更为简易。&lt;/p&gt;
&lt;h2 id=&#34;连续属性离散化&#34;&gt;&lt;a href=&#34;#连续属性离散化&#34; class=&#34;headerlink&#34; title=&#34;连续属性离散化&#34;&gt;&lt;/a&gt;连续属性离散化&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;确定连续属性的取值范围，确定划分点集合&lt;br&gt;考虑包含 n-1 个元素的候选划分点集合：&lt;br&gt;$$&lt;br&gt;T_a &amp;#x3D; \left{ \frac{a^i + a^{i+1}}{2} ,\middle|, 1 \leq i \leq n - 1 \right}&lt;br&gt;$$&lt;br&gt;这里的每个候选划分点是相邻两个取值的中点，即区间 $[a^i, a^{i+1})$ 的中位点 $\frac{a^i + a^{i+1}}{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算信息增益&lt;br&gt;$$&lt;br&gt;\text{Gain}(D, a, t) &amp;#x3D; \text{Ent}(D) - \sum_{\lambda \in {-, +}} \frac{|D_t^\lambda|}{|D|} \cdot \text{Ent}(D_t^\lambda)&lt;br&gt;$$&lt;br&gt;计算每个划分点的信息增益率，选择信息增益最大的划分点&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;+++info example&lt;br&gt;;;;id3 example&lt;br&gt;给定数据点及其对应的类别标签如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a_1 &amp;#x3D; 1$ -&amp;gt; 类别为 0&lt;/li&gt;
&lt;li&gt;$a_2 &amp;#x3D; 3$ -&amp;gt; 类别为 1&lt;/li&gt;
&lt;li&gt;$a_3 &amp;#x3D; 5$ -&amp;gt; 类别为 0&lt;/li&gt;
&lt;li&gt;$a_4 &amp;#x3D; 7$ -&amp;gt; 类别为 1&lt;/li&gt;
&lt;li&gt;$a_5 &amp;#x3D; 9$ -&amp;gt; 类别为 0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，我们的数据集 $D$ 是 ${1, 3, 5, 7, 9}$，对应的类别标签分别为 ${0, 1, 0, 1, 0}$。&lt;/p&gt;
&lt;p&gt;第一步：计算原始数据集的信息熵&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;Ent(D) &amp;#x3D; -\left( p_0 \log_2 p_0 + p_1 \log_2 p_1 \right)&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;其中，$p_0 &amp;#x3D; \frac{3}{5}$，$p_1 &amp;#x3D; \frac{2}{5}$，则：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;Ent(D) &amp;#x3D; -\left( \frac{3}{5} \log_2 \frac{3}{5} + \frac{2}{5} \log_2 \frac{2}{5} \right) \approx 0.971&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;第二步：确定候选划分点集合&lt;/p&gt;
&lt;p&gt;根据公式 $T_a &amp;#x3D; \left{ \frac{a^i + a^{i+1}}{2} ,\middle|, 1 \leq i \leq n - 1 \right}$，我们得到候选划分点集合：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;T_a &amp;#x3D; {2, 4, 6, 8}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;第三步：计算每个候选划分点的信息增益&lt;/p&gt;
&lt;p&gt;以划分点 $t &amp;#x3D; 4$ 为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$D_t^{-} &amp;#x3D; {1, 3}$，类别为 ${0, 1}$&lt;/li&gt;
&lt;li&gt;$D_t^{+} &amp;#x3D; {5, 7, 9}$，类别为 ${0, 1, 0}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;计算这两个子集的熵：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$$Ent(D_t^{-}) &amp;#x3D; -\left( \frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{2} \log_2 \frac{1}{2} \right) &amp;#x3D; 1$$&lt;/li&gt;
&lt;li&gt;$$Ent(D_t^{+}) &amp;#x3D; -\left( \frac{2}{3} \log_2 \frac{2}{3} + \frac{1}{3} \log_2 \frac{1}{3} \right) \approx 0.918$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;计算信息增益：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;Gain(D, a, t&amp;#x3D;4) &amp;#x3D; Ent(D) - \left( \frac{|D_t^{-}|}{|D|} \cdot Ent(D_t^{-}) + \frac{|D_t^{+}|}{|D|} \cdot Ent(D_t^{+}) \right)&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;代入数值：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;Gain(D, a, t&amp;#x3D;4) &amp;#x3D; 0.971 - \left( \frac{2}{5} \cdot 1 + \frac{3}{5} \cdot 0.918 \right) \approx 0.029&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;重复上述过程，对所有划分点 $t &amp;#x3D; 2, 4, 6, 8$ 进行类似计算，并选择使 $Gain(D, a, t)$ 最大的那个作为最优划分点。&lt;br&gt;;;;&lt;br&gt;+++&lt;/p&gt;
&lt;h1 id=&#34;线性区别分析-LDA-FDA&#34;&gt;&lt;a href=&#34;#线性区别分析-LDA-FDA&#34; class=&#34;headerlink&#34; title=&#34;线性区别分析 (LDA&amp;#x2F;FDA)&#34;&gt;&lt;/a&gt;线性区别分析 (LDA&amp;#x2F;FDA)&lt;/h1&gt;&lt;p&gt;线性判别分析(linear discriminant analysis， LDA)是一种基于监督学习的降维方法，也称为Fisher线性判别分析(fisher’s discriminant analysis，FDA),对于一组具有标签信息的高维数据样本，LDA利用其类别信息，将其线性投影到一个低维空间上，在低维空间中同一类别样本尽可能靠近，不同类别样本尽可能彼此远离。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算数据样本集中每个类别样本的均值&lt;/li&gt;
&lt;li&gt;计算类内散度矩阵$S_w$和类间散度矩阵$S_b$&lt;/li&gt;
&lt;li&gt;根据$S_w^{-1}S_bW&amp;#x3D;\lambda W$来求解$S_w^{-1}S_b$所对应前$r$个最大特征值所对应特征向量$(w_1,w_2,…,w_r)$，构成矩阵W&lt;/li&gt;
&lt;li&gt;通过矩阵$W$将每个样本映射到低维空间，实现特征降维。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体不想看，考到就给了&lt;/p&gt;
&lt;h1 id=&#34;Ada-Boosting&#34;&gt;&lt;a href=&#34;#Ada-Boosting&#34; class=&#34;headerlink&#34; title=&#34;Ada Boosting&#34;&gt;&lt;/a&gt;Ada Boosting&lt;/h1&gt;&lt;p&gt;。。看不懂懒得看&lt;/p&gt;
&lt;h1 id=&#34;支持向量机&#34;&gt;&lt;a href=&#34;#支持向量机&#34; class=&#34;headerlink&#34; title=&#34;支持向量机&#34;&gt;&lt;/a&gt;支持向量机&lt;/h1&gt;&lt;h1 id=&#34;生成学习模型&#34;&gt;&lt;a href=&#34;#生成学习模型&#34; class=&#34;headerlink&#34; title=&#34;生成学习模型&#34;&gt;&lt;/a&gt;生成学习模型&lt;/h1&gt;</content>
        <category term="人工智能" />
        <updated>2025-06-11T16:00:00.000Z</updated>
    </entry>
    <entry>
        <id>http://example.com/2025/06/11/AI/week4-5/</id>
        <title>Week4-5</title>
        <link rel="alternate" href="http://example.com/2025/06/11/AI/week4-5/"/>
        <content type="html">&lt;p&gt;#Ch3 搜索算法&lt;/p&gt;
&lt;h2 id=&#34;无信息搜索&#34;&gt;&lt;a href=&#34;#无信息搜索&#34; class=&#34;headerlink&#34; title=&#34;无信息搜索&#34;&gt;&lt;/a&gt;无信息搜索&lt;/h2&gt;&lt;p&gt;BFS DFS 略&lt;/p&gt;
&lt;h2 id=&#34;启发式搜索&#34;&gt;&lt;a href=&#34;#启发式搜索&#34; class=&#34;headerlink&#34; title=&#34;启发式搜索&#34;&gt;&lt;/a&gt;启发式搜索&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;贪婪优先搜索&lt;ul&gt;
&lt;li&gt;每次取最短的；缺点：不一定是最优的&lt;/li&gt;
&lt;li&gt;时间和空间复杂度均为 $O(b_m)$，b是搜索树分支因子，m是最大深度&lt;br&gt;&lt;img data-src=&#34;/figure1.png&#34;&gt;&lt;br&gt;:::info&lt;br&gt;每次取当前节点的下一个节点到终点中直线距离最短的&lt;br&gt;:::&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A*算法&lt;ul&gt;
&lt;li&gt;评价函数：f(n) &amp;#x3D; g(n) + h(n)&lt;/li&gt;
&lt;li&gt;代价函数 g(n) 表示从起始结点到结点n的开销代价值&lt;/li&gt;
&lt;li&gt;启发函数 h(n) 表示从结点n到目标结点路径中所估算的最小开销代价值。&lt;/li&gt;
&lt;li&gt;评价函数 f(n) 可视为经过结点n、具有最小开销代价值的路径。&lt;ul&gt;
&lt;li&gt;在最短路径问题中，g(?)为当前选择的路径的实际距离，即从上一个节点到下一个节点的实际距离，?(?)为下一个节点到目标城市的直线距离。每一次搜索，下一个节点选择与此刻城市连接的所有节点中，g(?)+?(?)最小的城市节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;:::info&lt;br&gt;取（当前节点到下一节点的距离+下一节点到目标城市的距离）最短的&lt;br&gt;:::&lt;br&gt;A*算法的完备性和最优性取决于搜索问题和启发函数的性质&lt;br&gt;一个良好的启发函数需要满足:可容性（admissible）;一致性（consistency）&lt;br&gt;如果启发函数是可容的，那么树搜索的A*算法满足最优性(最优性:搜索算法是否能保证找到的第一个解是最优解)&lt;br&gt;满足一致性条件的启发函数一定满足可容性条件，反之不一定&lt;/p&gt;
&lt;h2 id=&#34;对抗搜索&#34;&gt;&lt;a href=&#34;#对抗搜索&#34; class=&#34;headerlink&#34; title=&#34;对抗搜索&#34;&gt;&lt;/a&gt;对抗搜索&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;最小最大搜索（minimax）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最小最大搜索是一个在你和对手轮流行动的情况下，为你自己寻找最优策略的算法。&lt;/li&gt;
&lt;li&gt;算法：略&lt;/li&gt;
&lt;li&gt;时间复杂度：$O(b^m)$&lt;/li&gt;
&lt;li&gt;空间复杂度：$O(bm)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\alpha-\beta剪枝&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimax 会穷举整个博弈树，但我们可以用剪枝技巧跳过一些无用分支，让它跑得更快&lt;/li&gt;
&lt;li&gt;max层的下界取下一层（上界）里面最大的；min层的上界取下一层（下界）里面最小的&lt;br&gt;懒得写直接看例子：&lt;br&gt;&lt;img data-src=&#34;/figure2.png&#34;&gt;&lt;br&gt; Alpha-Beta 剪枝算法什么时候扩展的结点数量最少？&lt;/li&gt;
&lt;li&gt;每一层最左端结点的所有孩子结点均被访问，其他节点仅有最左端孩子结点被访问、其他孩子结点被剪枝。&lt;br&gt; 如果一个节点导致了其兄弟节点被剪枝，可知其孩子节点必然被扩展。&lt;/li&gt;
&lt;li&gt;最优效率下时间复杂度：$O(b^{m&amp;#x2F;2})$  (或者m+1);最差的就是完全没剪枝和minimax一样&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;蒙特卡洛树搜索&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择(UCB)、扩展(随机)、模拟(随机)、反向传播&lt;/li&gt;
&lt;li&gt;悔值函数&lt;br&gt;:::info&lt;br&gt;没完全懂，后面再回来研究&lt;br&gt;:::&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="人工智能" />
        <updated>2025-06-10T16:00:00.000Z</updated>
    </entry>
    <entry>
        <id>http://example.com/2025/05/20/AI/week1/</id>
        <title>Week1</title>
        <link rel="alternate" href="http://example.com/2025/05/20/AI/week1/"/>
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;2025-2026春夏人工智能课程笔记&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;Ch1-绪论&#34;&gt;&lt;a href=&#34;#Ch1-绪论&#34; class=&#34;headerlink&#34; title=&#34;Ch1 绪论&#34;&gt;&lt;/a&gt;Ch1 绪论&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;人工智能求解：&lt;ul&gt;
&lt;li&gt;以符号主义为核心的逻辑推理：将概念（如命题等）符号化，从若干判断（前提）出发得到新判断（结论）&lt;/li&gt;
&lt;li&gt;以问题求解为核心的探寻搜索:探寻搜索依据已有信息来寻找满足约束条件的待求解问题的答案&lt;/li&gt;
&lt;li&gt;以数据驱动为核心的机器学习:从数据中发现数据所承载语义（如概念）的内在模式&lt;/li&gt;
&lt;li&gt;以行为主义为核心的强化学习:根据环境所提供的奖罚反馈来学习所处状态可施加的最佳行动，在“探索（未知空间）-利用（已有经验）（exploration vs. exploitation）”之间寻找平衡，完成某个序列化任务，具备自我学习能力&lt;/li&gt;
&lt;li&gt;以博弈对抗为核心的群体智能:从“数据拟合”优化解的求取向“均衡解”的求取迈进&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="人工智能" />
        <updated>2025-05-19T16:00:00.000Z</updated>
    </entry>
    <entry>
        <id>http://example.com/2025/05/20/AI/week2-3/</id>
        <title>Week2-3</title>
        <link rel="alternate" href="http://example.com/2025/05/20/AI/week2-3/"/>
        <content type="html">&lt;h1 id=&#34;Ch2-知识表达与推理&#34;&gt;&lt;a href=&#34;#Ch2-知识表达与推理&#34; class=&#34;headerlink&#34; title=&#34;Ch2 知识表达与推理&#34;&gt;&lt;/a&gt;Ch2 知识表达与推理&lt;/h1&gt;&lt;h2 id=&#34;命题逻辑&#34;&gt;&lt;a href=&#34;#命题逻辑&#34; class=&#34;headerlink&#34; title=&#34;命题逻辑&#34;&gt;&lt;/a&gt;命题逻辑&lt;/h2&gt;&lt;p&gt;&lt;img data-src=&#34;/img1.png&#34;&gt;&lt;br&gt;真值表：&lt;br&gt;&lt;img data-src=&#34;/img2.png&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“条件”命题联结词中前提为假时命题结论永远为真，bi-conditional只有两个都是true或者都是false才是true&lt;br&gt;逻辑等价：给定命题p和命题q，如果&amp;#x3D;&amp;#x3D;p和q在所有情况下都具有同样真假结果&amp;#x3D;&amp;#x3D;，那么p和q在逻辑上等价，一般用 $\equiv$ 来表示，即p $\equiv$ q。&lt;br&gt;判断逻辑等价：画真值表&lt;br&gt;逻辑等价式：&lt;br&gt;&lt;img data-src=&#34;/img3.jpg&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img4.png&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;normal form&lt;ul&gt;
&lt;li&gt;有限个简单合取式构成的析取式称为析取(or)范式&lt;/li&gt;
&lt;li&gt;由有限个简单析取式构成的合取式称为合取(and)范式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;谓词逻辑&#34;&gt;&lt;a href=&#34;#谓词逻辑&#34; class=&#34;headerlink&#34; title=&#34;谓词逻辑&#34;&gt;&lt;/a&gt;谓词逻辑&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;全称量词与存在量词&lt;/li&gt;
&lt;li&gt;约束变元、自由变元&lt;br&gt;:::info&lt;br&gt;在约束变元相同的情况下，量词的运算满足分配律：全称量词对析取没有分配律、存在量词对合取没有分配律&lt;br&gt;:::&lt;br&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(A(x) \lor B(x)) \equiv (\forall x)A(x) \lor (\forall x)B(x) 不成立&lt;br&gt;\end{aligned}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(A(x) \land B(x)) \equiv (\forall x)A(x) \land (\forall x)B(x) 成立&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists x)(A(x) \lor B(x)) \equiv (\exists x)A(x) \lor (\exists x)B(x) 成立&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists x)(A(x) \land B(x)) \equiv (\exists x)A(x) \land (\exists x)B(x) 不成立&lt;br&gt;\end{aligned}$$&lt;br&gt;:::info&lt;br&gt;当公式中存在多个量词时，若多个量词都是全称量词或者都是存在量词，则量词的位置可以互换；若多个量词中既有全称量词又有存在量词，则量词的位置不可以随意互换&lt;br&gt;:::&lt;br&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(\forall y)A(x, y) \equiv (\forall y)(\forall x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists x)(\exists y)A(x, y) \equiv (\exists y)(\exists x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(\forall y)A(x, y) \equiv (\exists y)(\forall x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(\forall y)A(x, y) \equiv (\exists x)(\forall y)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists y)(\forall x)A(x, y) \equiv (\forall x)(\exists y)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\exists x)(\forall y)A(x, y) \equiv (\forall y)(\exists x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall x)(\exists y)A(x, y) \equiv (\exists y)(\exists x)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;(\forall y)(\exists x)A(x, y) \equiv (\exists x)(\exists y)A(x, y)&lt;br&gt;\end{aligned}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;利用谓词逻辑进行推理&lt;ul&gt;
&lt;li&gt;全称量词消去： $(\forall x) A(x) \equiv A(y)$&lt;/li&gt;
&lt;li&gt;全称量词引入： $A(y) \equiv (\forall x) A(x)$&lt;/li&gt;
&lt;li&gt;存在量词消去： $(\exists x) A(x) \equiv A(c)$&lt;/li&gt;
&lt;li&gt;存在量词引入： $A(c) \equiv (\exists x) A(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;知识图谱推理&#34;&gt;&lt;a href=&#34;#知识图谱推理&#34; class=&#34;headerlink&#34; title=&#34;知识图谱推理&#34;&gt;&lt;/a&gt;知识图谱推理&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;知识图谱可视为包含多种关系的图。在图中，每个节点是一个实体（如人名、地名、事件和活动等），任意两个节点之间的边表示这两个节点之间存在的关系。&lt;/li&gt;
&lt;li&gt;可将知识图谱中任意两个相连节点及其连接边表示成一个三元组（triplet）,即 (left_node, relation, right_node)&lt;br&gt;两类代表性方法：&lt;/li&gt;
&lt;li&gt;归纳逻辑程序设计 (inductive logic programming，ILP)算法&lt;/li&gt;
&lt;li&gt;路径排序算法（path ranking algorithm, PRA）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ILP: 一阶归纳学习FOIL（First Order Inductive Learner）&lt;br&gt;推理手段: 正例集合 + 反例集合 + 背景知识样例 ⟹ 目标谓词作为结论的推理规则&lt;br&gt;&lt;img data-src=&#34;/img5.png&#34;&gt;&lt;br&gt;懒得写了，看ppt吧&lt;br&gt;&lt;img data-src=&#34;/img6.png&#34;&gt;&lt;br&gt;推理规则覆盖所有正例且不覆盖任何反例的时候算法结束&lt;/p&gt;
&lt;p&gt;PRA: 路径排序算法&lt;br&gt;&lt;img data-src=&#34;/img7.png&#34;&gt;&lt;br&gt;(4)的意思是看两个实体能不能通过(3)的关系从第一个走到第二个。&lt;br&gt;后面的1表示正例，-1表示负例。&lt;/p&gt;
&lt;h2 id=&#34;概率图推理&#34;&gt;&lt;a href=&#34;#概率图推理&#34; class=&#34;headerlink&#34; title=&#34;概率图推理&#34;&gt;&lt;/a&gt;概率图推理&lt;/h2&gt;&lt;p&gt;贝叶斯网络&lt;br&gt;马尔科夫逻辑网络&lt;/p&gt;
&lt;h2 id=&#34;因果推理&#34;&gt;&lt;a href=&#34;#因果推理&#34; class=&#34;headerlink&#34; title=&#34;因果推理&#34;&gt;&lt;/a&gt;因果推理&lt;/h2&gt;&lt;p&gt;因果定义：变量X是变量Y的原因，当且仅当保持其它所有变量不变的情况下，改变X的值能导致Y的值发生变化。&lt;br&gt;因果效应：因变量X改变一个单位时，果变量Y的变化程度&lt;/p&gt;
&lt;p&gt;因果图是有向无环图&lt;/p&gt;
&lt;p&gt;结构因果模型：结构因果模型由两组变量集合U和V以及一组函数f组成。其中，f是根据模型中其他变量取值而给V中每一个变量赋值的函数&lt;br&gt;结构因果模型中的原因：如果变量X出现在给变量X赋值的函数中，如$Y &amp;#x3D; f(X) + \epsilon$，则X是Y的直接原因&lt;br&gt;因果图中的联合概率分布：直接看图&lt;br&gt;&lt;img data-src=&#34;/img8.png&#34;&gt;&lt;br&gt;因果图的基本结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;链结构&lt;br&gt;  - &lt;img data-src=&#34;/img9.png&#34;&gt;&lt;br&gt;  - 对于变量X和Y，若X和Y之间只有一条单向的路径，变量Z是截断(intercept)该路径的集合中的任一变量，则在给定Z时，X和Y条件独立。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$&lt;br&gt;P(X, Y | Z) &amp;#x3D; P(X | Z)P(Y | Z)&lt;br&gt;$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分连结构&lt;br&gt;  - &lt;img data-src=&#34;/img10.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$&lt;br&gt;P(X, Y | Z) &amp;#x3D; \frac {P(X, Y, Z)}{P(Z)} &amp;#x3D; \frac {P(X | Z)P(Y | Z)P(Z)}{P(Z)} &amp;#x3D; P(X | Z)P(Y | Z)&lt;br&gt;$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;汇联结构&lt;br&gt;  - &lt;img data-src=&#34;/img11.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$&lt;br&gt;P(X, Y | Z) &amp;#x3D; \frac{P(X, Y, Z)} {P(Z)} &amp;#x3D; \frac {P(X, Y, Z)}{P(Z)} &amp;#x3D; \frac {P(X)P(Y)P(Z&amp;#x2F;X, Y)}{P(Z)} \neq P(X | Z)P(Y | Z)&lt;br&gt;$$&lt;/p&gt;
&lt;h3 id=&#34;D-分离-directional-separation-d-separation-，可用于判断任意两个节点的相关性和独立性&#34;&gt;&lt;a href=&#34;#D-分离-directional-separation-d-separation-，可用于判断任意两个节点的相关性和独立性&#34; class=&#34;headerlink&#34; title=&#34;D-分离(directional separation, d-separation)，可用于判断任意两个节点的相关性和独立性&#34;&gt;&lt;/a&gt;D-分离(directional separation, d-separation)，可用于判断任意两个节点的相关性和独立性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;限定集：已知或观察到的变量集合（给定的变量集合）&lt;/li&gt;
&lt;li&gt;路径p被限定集Z阻塞(block)当且仅当：&lt;ul&gt;
&lt;li&gt;(1) 路径p含有链结构A → B → C或分连结构A ← B → C且中间节点B在Z中，或&lt;/li&gt;
&lt;li&gt;(2) 路径p含有汇连结构A → B ← C且汇连节点B及其后代都不在Z中。&lt;/li&gt;
&lt;li&gt;若Z阻塞了节点X和节点Y之间的每一条路径，则称给定Z时，X和Y是D-分离，即给定Z时，X和Y条件独立&lt;/li&gt;
&lt;li&gt;&amp;#x3D;&amp;#x3D;链式、分连中间节点在，汇联中间节点和后代不在则D-分离&amp;#x3D;&amp;#x3D;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因果定义：变量X是变量Y的原因，当且仅当保持其它所有变量不变的情况下，改变X的值能导致Y的值发生变化。&lt;br&gt;因果效应：因变量X改变一个单位时，果变量Y的变化程度因果推理的两个关键因素：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;改变因变量T&lt;/li&gt;
&lt;li&gt;保证其它变量不变&lt;br&gt;干预：干预(intervention)指的是固定(fix)系统中的变量，然后改变系统，观察其他变量的变化。&lt;br&gt;为了与X自然取值x时进行区分，在对X进行干预时，引入“do算子”(do-calculus)，记作do(X &amp;#x3D; x)。&lt;br&gt;因此，P(Y &amp;#x3D; y|X &amp;#x3D; x)表示的是当发现X &amp;#x3D; x时，Y&amp;#x3D; y的概率；而P(Y &amp;#x3D; y|do(X &amp;#x3D;x))表示的是对X进行干预，固定其值为x时，Y &amp;#x3D; y的概率。&lt;br&gt;用统计学的术语来说，P(Y &amp;#x3D; y|X &amp;#x3D; x)反映的是在取值为x的个体X上，Y的总体分布；而P(Y &amp;#x3D; y|do(X &amp;#x3D;x))反映的是如果将每一个X取值都固定为x时，Y的总体分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因果效应差&amp;#x2F;平均因果效应 (ACE)  懒得写了看图吧&lt;br&gt;&lt;img data-src=&#34;/img12.png&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img13.png&#34;&gt;&lt;br&gt;计算因果效应的关键在于计算操纵概率(manipulatedprobability) $P_m$&lt;br&gt;调整公式：&lt;br&gt;$$&lt;br&gt;P(Y &amp;#x3D; y \mid do(X &amp;#x3D; x)) &amp;#x3D; \sum_z P(Y &amp;#x3D; y \mid X &amp;#x3D; x, Z &amp;#x3D; z) \cdot P(Z &amp;#x3D; z)&lt;br&gt;$$&lt;br&gt;对于Z的每一个取值z，计算X和Y的条件概率并取均值&lt;br&gt;+++info example&lt;br&gt;;;;id3 例题&lt;br&gt;假设我们研究以下变量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X：是否服药  &lt;ul&gt;
&lt;li&gt;$X &amp;#x3D; 1$：服药  &lt;/li&gt;
&lt;li&gt;$X &amp;#x3D; 0$：不服药&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Y：是否康复  &lt;ul&gt;
&lt;li&gt;$Y &amp;#x3D; 1$：康复  &lt;/li&gt;
&lt;li&gt;$Y &amp;#x3D; 0$：未康复&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Z：性别  &lt;ul&gt;
&lt;li&gt;$Z &amp;#x3D; 0$：男  &lt;/li&gt;
&lt;li&gt;$Z &amp;#x3D; 1$：女&lt;br&gt;我们知道性别会影响：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;是否选择服药（比如男性更倾向于尝试新药）&lt;/li&gt;
&lt;li&gt;康复率（比如女性可能有更强的免疫力）&lt;br&gt;因此，性别 Z 是一个混杂变量，需要在分析中进行控制。&lt;br&gt;已知：&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Z（性别）&lt;/th&gt;
&lt;th&gt;P(Z)&lt;/th&gt;
&lt;th&gt;P(Y&amp;#x3D;1 | X&amp;#x3D;1, Z)&lt;/th&gt;
&lt;th&gt;P(Y&amp;#x3D;1 | X&amp;#x3D;0, Z)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;男（0）&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;女（1）&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;我们想知道：&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;如果强制所有人都服药（即 $do(X&amp;#x3D;1)$），整体康复率是多少？&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;也就是要计算：&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;P(Y&amp;#x3D;1 \mid do(X&amp;#x3D;1))&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;;;;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;;;;id3 答案&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;根据调整公式：&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$&lt;br&gt;P(Y&amp;#x3D;1 \mid do(X&amp;#x3D;1)) &amp;#x3D; \sum_z P(Y&amp;#x3D;1 \mid X&amp;#x3D;1, Z&amp;#x3D;z) \cdot P(Z&amp;#x3D;z)&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;代入数据计算&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(Y&amp;#x3D;1 \mid do(X&amp;#x3D;1)) &amp;#x3D; P(Y&amp;#x3D;1 \mid X&amp;#x3D;1, Z&amp;#x3D;0) \cdot P(Z&amp;#x3D;0) + P(Y&amp;#x3D;1 \mid X&amp;#x3D;1, Z&amp;#x3D;1) \cdot P(Z&amp;#x3D;1)&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;&amp;#x3D; 0.7 \times 0.6 + 0.5 \times 0.4 &amp;#x3D; 0.42 + 0.2 &amp;#x3D; 0.62&lt;br&gt;$$&lt;br&gt;+++&lt;/p&gt;
&lt;p&gt;(因果效应)给定因果图G，PA表示X的父节点集合，则X对Y的因果效应为&lt;br&gt;$$&lt;br&gt;P(Y&amp;#x3D;y \mid do(X&amp;#x3D;x)) &amp;#x3D; \sum_z P(Y&amp;#x3D;y \mid X&amp;#x3D;x, PA&amp;#x3D;z) \cdot P(PA&amp;#x3D;z)&lt;br&gt;$$&lt;br&gt;后门调整：&lt;br&gt;不写了&lt;/p&gt;
</content>
        <category term="人工智能" />
        <updated>2025-05-19T16:00:00.000Z</updated>
    </entry>
</feed>
